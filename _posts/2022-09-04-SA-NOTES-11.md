---
layout: article
title: Shape Analysis课程笔记11-Structure Preserving Embedding
tags: ["Shape Analysis", "Geometry Processing", "CG"]
key: SA-11
aside:
  toc: true
sidebar:
  nav: SA
---

> 这个系列是[MIT 6.838: Shape Analysis](https://groups.csail.mit.edu/gdpgroup/6838_spring_2021.html)的同步课程笔记。本课程会介绍几何方法在图形学、机器学习、计算机视觉、医疗图像以及建筑设计等相关领域的原理和应用。本节主要介绍流形上的嵌入问题。
<!--more-->

## Intrinsic-to-Extrinsic Embedding

在上一节课中我们主要介绍了如何在欧式空间中通过度量来进行嵌入。在很多现实问题中我们并不关心全局的信息，而是主要考量每个样本和它邻域的局部关系。这样通过局部信息来进行嵌入的问题称为**intrinsic embedding**。

<div align=center>
<img src="https://i.imgur.com/nyJnGpi.png" width="80%">
</div>

实际上这种局部嵌入和全局嵌入是相互联系的，人们已经在理论上证明流形可以嵌入到更高维的空间中。

<div align=center>
<img src="https://i.imgur.com/PyB84EW.png" width="80%">
</div>

## ISOMAP

流形的嵌入是一个非常有研究价值的问题，比如在机器学习领域人们发现很多数据实际上是一个嵌入在高维空间中的流形。和直接在高维空间中进行处理相比，在流形上研究数据的分布往往会得到很多更本质的认识。计算高维数据在流形上分布的经典算法是**ISOMAP**，它与上一节介绍过的[MDS](/2022/08/28/SA-NOTES-10.html#classical-mds)几乎是完全一致的，只是在构造距离矩阵时需要使用图上的最短路径而不是直接使用高维空间中的距离。

<div align=center>
<img src="https://i.imgur.com/JY0YVP2.png" width="80%">
</div>

在计算图上任意节点之间的最短路径时可以使用Dijkstra算法对所有节点进行遍历。不过这里更推荐使用**Floyd-Warsahll算法**，它可以直接计算出所有节点对的最短路径。

<div align=center>
<img src="https://i.imgur.com/gBw5SOk.png" width="80%">
</div>

类似于[Landmark MDS](/2022/08/28/SA-NOTES-10.html#landmark-mds)，只要对ISOMAP稍加改造就可以得到**Landmark ISOMAP**，它对于增量式的问题有更好的性能。

<div align=center>
<img src="https://i.imgur.com/7SQcrQ8.png" width="80%">
</div>

## Locally-Linear Embedding

除了ISOMAP之外另一种处理流形嵌入的经典算法是**locally-linear embedding(LLE)**，它的思想是使用样本的邻域来进行嵌入并保证嵌入前后局部的权重是一样的。因此LLE的主要流程可以分为analysis step和embedding step两步：在analysis step中计算每个样本和它邻域的权重，而在embedding step则计算样本在给定权重下的嵌入。

<div align=center>
<img src="https://i.imgur.com/UBEwssl.png" width="80%">
</div>

### Analysis Step

analysis step的目标是对于任意样本$\mathbf{x}_i$计算它和$k$个相邻样本的权重，它可以表述为如下的优化问题：

$$
\begin{aligned}
\min_{\omega^1, ..., \omega^k} \ &\Vert \mathbf{x}_i - \sum_j \omega^j \mathbf{n}_j \Vert_2^2 \\
\text{s.t.} \ & \sum_j \omega^j = 1
\end{aligned}
$$

对于这样的约束优化问题，我们可以使用[Lagrangian乘子法](/2022/08/04/SA-NOTES-02.html#lagrangian-multiplier)来转换成求解线性系统：

$$
\begin{bmatrix}
\mathbf{N}^T \mathbf{N} & \mathbf{1} \\
\mathbf{1}^T & \mathbf{0}
\end{bmatrix}
\begin{bmatrix}
\omega \\ \lambda
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{N}^T \mathbf{x}_i \\ 1
\end{bmatrix}
$$

$$
\mathbf{N} = 
\begin{bmatrix}
\mathbf{n}_1 & \mathbf{n}_2 & \dots & \mathbf{n}_k
\end{bmatrix}
$$

因此在analysis step中我们只需要对样本进行遍历，通过寻找它的$k$个邻居然后构造并求解线性系统即可。

<div align=center>
<img src="https://i.imgur.com/mzJVDPw.png" width="80%">
</div>

### Embedding Step

在embedding step中我们需要计算样本$\mathbf{x}_i$的一个低维嵌入$\mathbf{y}_i$同时保持它和邻域之间的局部权重。因此我们可以构造出优化问题：

$$
\min_\mathbf{Y} \ \Vert \mathbf{Y} - \mathbf{Y} W^T \Vert_F^2
$$

其中$W$是analysis step计算得到的样本权重矩阵。注意到上式的解包含平凡解$\mathbf{Y} = \mathbf{0}$，为了避免这种情况我们需要引入额外的约束条件：

$$
\begin{aligned}
\min_\mathbf{Y} \ & \Vert \mathbf{Y} - \mathbf{Y} W^T \Vert_F^2 \\
\text{s.t.} \ & \mathbf{Y} \mathbf{Y}^T = \mathbf{I} \\
& \mathbf{Y} \mathbb{1} = \mathbb{0}
\end{aligned}
$$

可以证明此时的优化问题等价于求解特征值问题：

$$
\begin{aligned}
\Vert \mathbf{Y} - \mathbf{Y} W^T \Vert_F^2 &= \Vert \mathbf{Y} (\mathbf{I} - W^T) \Vert_F^2 \\
&= \text{tr} \big( \mathbf{Y} (\mathbf{I} - W^T) (\mathbf{I} - W) \mathbf{Y}^T \big) \\
&= \text{tr} ( \mathbf{Y} ( \mathbf{I} - W - W^T + W^T W ) \mathbf{Y}^T ) \\
&= \text{tr} ( \mathbf{Y} \mathbf{M} \mathbf{Y}^T )
\end{aligned} 
$$

此时的优化问题可以表示为：

$$
\begin{aligned}
\min_\mathbf{Y} \ & \text{tr} ( \mathbf{Y} \mathbf{M} \mathbf{Y}^T ) \\
\text{s.t.} \ & \mathbf{Y} \mathbf{Y}^T = \mathbf{I} \\
& \mathbf{Y} \mathbb{1} = \mathbb{0}
\end{aligned}
$$

而它的解是取$\mathbf{Y}$为$\mathbf{M}$的$p$个最小非零特征值。

<div align=center>
<img src="https://i.imgur.com/hBiGXol.png" width="80%">
</div>

### ISOMAP vs. LLE

ISOMAP和LLE最主要的区别在于ISOMAP需要利用全局信息来构建图，这导致它容易收到样本在高维空间分布形式以及噪声的影响。相对地LLE更多地利用了样本的局部信息，因此LLE会更稳定一些。

<div align=center>
<img src="https://i.imgur.com/aqtqTkW.png" width="80%">
</div>

### Other Options

除了上面介绍的两种方法外还可以使用**diffusion map**来计算样本嵌入。关于diffusion map的细节我们会等到Laplace算子以及heat equation中再进行介绍。

<div align=center>
<img src="https://i.imgur.com/mpssTxT.png" width="80%">
</div>

而在几何处理中样本嵌入的一个重要应用是计算**网格参数化(mesh parameterization)**。简单来说，网格参数化的目的是把一个三维的曲面映射到二维平面上同时维持曲面在三维空间中的一些性质。

<div align=center>
<img src="https://i.imgur.com/JGmDwcu.png" width="80%">
</div>

还有一些研究考虑使用测地距离而不是欧氏距离来计算样本嵌入。

<div align=center>
<img src="https://i.imgur.com/dbGEPNH.png" width="80%">
</div>

最后需要说明的是目前人们已经开发出了非常多的嵌入技术，在使用时可以根据实际问题的需要来进行选择。

<div align=center>
<img src="https://i.imgur.com/nT5dNeF.png" width="80%">
</div>

## Reference

- [Lecture 11：Structure-preserving embedding (ISOMAP, LLE); manifold learning](https://www.youtube.com/watch?v=VIWRQs1HYo8&list=PLQ3UicqQtfNtUcdTMLgKSTTOiEsCw2VBW&index=15&ab_channel=JustinSolomon)
- [Structure-Preserving Embedding](https://groups.csail.mit.edu/gdpgroup/assets/6838_spring_2021/7_inverse_distance_problems.pdf#page=38)
- [Chapter 7：Inverse distance problems](https://groups.csail.mit.edu/gdpgroup/assets/6838_spring_2021/chapter7.pdf)