---
layout: article
title: 深蓝学院-GNN课程笔记12-图神经网络的高级方法
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-12
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/course/376?source=1)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍现代图神经网络的研究热点。
<!--more-->

## 更深的图神经网络

现代计算机视觉和自然语言处理等任务中往往会使用深层的神经网络来提高模型的性能，然而在图神经网络中人们发现一味地加深模型的深度不仅不会提高性能反而会使模型的性能下降。

<div align=center>
<img src="https://i.imgur.com/3hNW5Jp.png" width="80%">
</div>

进一步对图上节点的嵌入进行可视化发现，随着图神经网络深度的增加图上节点的嵌入会趋于一致。从message passing的角度上看，每一层中图上节点会收集相邻节点的信息，而随着网络层数的加深每个节点收集到的信息都相当于在全图上进行信息融合。这样的现象称为**过度平滑(over smoothing)**。

<div align=center>
<img src="https://i.imgur.com/OZgLiWa.png" width="80%">
</div>

我们可以从数学上来严格解释过渡平滑的原因。以最基本的GCN为例，在不考虑激活函数的情况下每一层的图卷积相当于对特征进行了一个线性变换。其中$\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$表示每个节点从相邻节点收集信息，而$\Theta$则表示对信息进行线性变换。把模型中的所有层组合到一起就得到了整个模型的前向输出。

<div align=center>
<img src="https://i.imgur.com/WExjPig.png" width="80%">
</div>

接下来对$\tilde{D}^{-\frac{1}{2}} \tilde{A}^L \tilde{D}^{-\frac{1}{2}}$进行特征分解，可以发现它只与$\tilde{A}$的特征值相关。由于$\tilde{A}$的特征值都位于(-1, 1]区间上，当层数$L$较大时$\tilde{A}^L$会收敛到$\text{diag}(1, 0, 0, ...)$上。因此对于任意特征$f$经过图卷积后得到的结果都是最大特征值对应特征向量$u_1$乘以一个系数。

<div align=center>
<img src="https://i.imgur.com/hPQ66ca.png" width="80%">
</div>

把上面的结论代回到原始的模型表达式中可以发现模型输出的每一列都是$u_1$乘以一个系数，因此每个节点(每一行)的节点表示都是非常相似的。

<div align=center>
<img src="https://i.imgur.com/DUO26dM.png" width="80%">
</div>

为了缓解过度平滑的问题人们开发了各种各样的方法：jumping knowledge将每一层的输出拼接到一起作为模型的最终输入，dropedge则是在每一训练周期中随机去掉图上的一些边，此外还可以利用正则化的方法来强制不相连的节点特征保持不同。

<div align=center>
<img src="https://i.imgur.com/cKiTcen.png" width="80%">
</div>

## 图上的自监督学习

## 图神经网络的表达性