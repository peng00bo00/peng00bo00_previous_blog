---
layout: article
title: 深蓝学院-GNN课程笔记07-图上的其他深度学习模型
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-07
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/course/376?source=1)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图神经网络之外的其它深度学习模型。
<!--more-->

## 图上的循环神经网络

**循环神经网络(recurrent neural networks, RNN)**是处理序列数据的常用模型，而在LSTM则是RNN中最为常用的变体，它通过引入**单元状态(cell state)**来解决RNN难以处理长序列的问题。RNN和LSTM的基本流程可参考[循环神经网络](/2021/09/11/GNN-NOTES-02.html#循环神经网络)一节。

<div align=center>
<img src="https://i.imgur.com/1JY67wK.png" width="70%">
</div>

接下来我们考虑把LSTM拓展到图结构数据中。假设图数据可以使用表示为一棵树，我们只需要让每个子节点向父节点传递自身的信息即可，此时LSTM的流程为：

$$
\tilde{h}^{(k)} = \sum_{v_j \in N_c(v_k)} h^{(j)}
$$

$$
i_k = \sigma(W_i x^{(k)} + U_i \tilde{h}^{(k)} + b_i)
$$

$$
o_k = \sigma(W_o x^{(k)} + U_o \tilde{h}^{(k)} + b_o)
$$

需要注意的是此时每个子节点$v_j$向父节点$v_k$传递的信息都有自身对应的遗忘门$f_{kj}$：

$$
f_{kj} = \sigma(W_f x^{(k)} + U_f h^{(j)} + b_f)
$$

在更新单元状态时需要将来自每个子节点的信息加权求和：

$$
\tilde{C}^{(k)} = \tanh ((W_c x^{(k)} + U_c \tilde{h}^{(k)} + b_c)
$$

$$
C^{(k)} = i_k \odot \tilde{C}^{(k)} + \sum_{v_j \in N_c(v_k)} f_{kj} \odot C^{(j)}
$$

$$
h^{(k)} = o_k \odot \tanh (C^{(k)})
$$

<div align=center>
<img src="https://i.imgur.com/wV7K7WV.png" width="70%">
</div>

不难发现Tree-LSTM与标准LSTM没有本质区别，只需要在隐状态以及单元状态的信息传播时需要将子节点传递的信息进行加权求和即可。

## 图上的自编码器

## 图上的变分自编码器

## 图上的生成对抗网络