---
layout: article
title: Shape Analysis课程笔记02-Linear and Variational Problems
tags: ["Shape Analysis", "CG"]
key: SA-02
aside:
  toc: true
sidebar:
  nav: SA
---

> 这个系列是[MIT 6.838: Shape Analysis](https://groups.csail.mit.edu/gdpgroup/6838_spring_2021.html)的同步课程笔记。本课程会介绍几何方法在图形学、机器学习、计算机视觉、医疗图像以及建筑设计等相关领域的原理和应用。本节主要复习课程所需的数学知识。
<!--more-->

## Linear Algebra

### Vector

线性代数是本课程最重要的工具之一，因此我们首先来复习线性代数中的相关内容。**向量(vector)**是线性代数中最基本的概念，在本课程中我们默认向量都是列向量，这样向量内积还有二次型可以使用向量转置来表示。

<div align=center>
<img src="https://i.imgur.com/a0r1oDI.png" width="80%">
</div>

更严格来说，向量的坐标是向量本身和基向量$\mathbf{e}_k$的内积。

<div align=center>
<img src="https://i.imgur.com/HemKJil.png" width="80%">
</div>

### Two Roles for Matrices

**矩阵(matrix)**是另一个非常重要的概念。尽管矩阵更常见的形式是数的方阵，但在本课程中我们会把矩阵更多地理解为满足如下关系的**线性算子(linear operator)**。更进一步还可以证明任意满足要求的线性算子和二次型都对应着一个矩阵。

<div align=center>
<img src="https://i.imgur.com/1In4avv.png" width="80%">
</div>

因此矩阵既可以表示线性变换也可以表示向量内积，我们在使用时需要加以注意。

<div align=center>
<img src="https://i.imgur.com/S9JYtJi.png" width="80%">
</div>

### Einstein Notation

在本课程中我们还会大量使用Einstein记号来表示求和的过程。

<div align=center>
<img src="https://i.imgur.com/8jALi3q.png" width="80%">
</div>

使用Einstein记号的一个好处在于我们可以显式区分线性映射以及内积：

$$
w^j = L_k^j v^k
$$

<div align=center>
<img src="https://i.imgur.com/BeaMzMb.png" width="80%">
</div>

$$
g(\mathbf{u}, \mathbf{v}) = u^k v^l g_{kl}
$$

$$
g_{kl} = g(\mathbf{e}_k, \mathbf{e}_l)
$$

<div align=center>
<img src="https://i.imgur.com/q2NXuV8.png" width="80%">
</div>

<div align=center>
<img src="https://i.imgur.com/J8kFZXy.png" width="80%">
</div>

从线性算子的角度来认识矩阵还可以帮助我们把线性代数的概念推广到无限维空间中。比如说类似于矩阵特征向量的概念，我们可以定义算子的特征函数。

<div align=center>
<img src="https://i.imgur.com/w47PEA0.png" width="80%">
<img src="https://i.imgur.com/wYxXov8.png" width="80%">
</div>

### Linear System of Equations

线性代数的另一大应用是求解线性系统$$A\mathbf{x} = \mathbf{b}$$。当矩阵$A$是可逆矩阵时，我们可以通过高斯消元等方法来求解线性方程组。这里需要额外注意的是在任何情况下我们都应该避免直接计算矩阵的逆阵。

<div align=center>
<img src="https://i.imgur.com/amPKF5x.png" width="80%">
<img src="https://i.imgur.com/5FueEID.png" width="80%">
</div>

除了线性方程组外，很多线性算子也可以表示为线性系统。此时同样可以利用相关的技术来进行求解。

<div align=center>
<img src="https://i.imgur.com/hwv6kvf.png" width="80%">
</div>

在求解线性系统时，如果我们已知系统的一些特性还可以利用这些特性来加速求解。

<div align=center>
<img src="https://i.imgur.com/FQAVQOr.png" width="80%">
</div>

目前常用的求解器可以分为直接求解或是迭代求解两类。

<div align=center>
<img src="https://i.imgur.com/l8Lmkb5.png" width="80%">
</div>

在几何问题中最常见的线性系统是稀疏系统。利用系统的稀疏性我们可以极大地减少存储空间，同时还可以加速求解过程。

<div align=center>
<img src="https://i.imgur.com/WZ7qHwg.png" width="80%">
</div>

当然在本课程中我们不需要亲自去实现一个线性系统求解器，像matlab等数值计算软件中都有非常成熟稳定的求解器以供调用。

<div align=center>
<img src="https://i.imgur.com/7wfHgSy.png" width="80%">
</div>

## Variational Calculus

### Optimization Terminology

在正式介绍variational calculus前我们先来复习一下优化问题。一般来说一个优化问题由优化目标、等式约束以及不等式约束三部分组成。

<div align=center>
<img src="https://i.imgur.com/U0tHy5q.png" width="80%">
<img src="https://i.imgur.com/Gt7Sheo.png" width="80%">
<img src="https://i.imgur.com/ZmC8vk9.png" width="80%">
</div>

### Differential

在本课程中我们会大量使用**微分(differential)**的技术来求解优化问题。我们定义函数$f$在$\mathbf{x}_0$处沿方向$\mathbf{v}$的微分为：

$$
d f_{\mathbf{x}_0} (\mathbf{v}) = \lim_{h \rightarrow 0} \frac{f(\mathbf{x}_0 + h \mathbf{v}) - f(\mathbf{x}_0)}{h}
$$

利用泰勒展开可以证明：

$$
d f_{\mathbf{x}_0} (\mathbf{v}) = \sum_k \frac{\partial f}{\partial x^k} \cdot v^k = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}
$$

因此我们可以把$df$看作是关于向量$\mathbf{v}$的一个线性算子。

<div align=center>
<img src="https://i.imgur.com/OidqiO2.png" width="80%">
</div>

### Notions from Calculus

除此之外，函数的梯度、Jacobian矩阵以及Hessian矩阵也都可以表示为矩阵的形式。

<div align=center>
<img src="https://i.imgur.com/Gbf7PB5.png" width="80%">
<img src="https://i.imgur.com/uuCswNG.png" width="80%">
<img src="https://i.imgur.com/djvuM9g.png" width="80%">
</div>

### Optimization to Root-Finding

对于无约束优化问题，我们可以利用梯度的概念将它转换为对梯度求根的问题。不过需要注意的是梯度的根只是优化问题的极值点，想要确认具体是极大值还是极小值还需要结合其他的条件。

<div align=center>
<img src="https://i.imgur.com/v0pqCuN.png" width="80%">
</div>

### Encapsulates Many Problems

实际上很多问题都可以转换为一个优化问题。

<div align=center>
<img src="https://i.imgur.com/MnJ1K2A.png" width="80%">
</div>

不过需要注意的是很多情况下使用优化工具来求解这些问题并不一定比直接求解原始问题来得高效。以二次型的优化问题为例，假设优化目标和约束为：

$$
\begin{aligned}
\min \ &\frac{1}{2} \mathbf{x}^T A \mathbf{x} - \mathbf{b}^T \mathbf{x} + c \\
\text{s.t.} \ &M \mathbf{x} = \mathbf{v}
\end{aligned}
$$

利用Lagrangian乘子法可以将其转换为一个无约束优化问题：

$$
\Lambda (\mathbf{x}, \lambda) = \frac{1}{2} \mathbf{x}^T A \mathbf{x} - \mathbf{b}^T \mathbf{x} + c + \lambda^T (M \mathbf{x} - \mathbf{v})
$$

分别对$\mathbf{x}$和$\lambda$求导可以得到：

$$
\nabla_{\mathbf{x}} = A \mathbf{x} - \mathbf{b} + M^T \lambda = 0
$$

$$
\nabla_{\lambda} = M \mathbf{x} - \mathbf{v} = 0
$$

整理后得到一个新的线性系统：

$$
\begin{bmatrix}
A & M^T \\
M & 0
\end{bmatrix}

\begin{bmatrix}
\mathbf{x} \\
\lambda
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{b} \\
\mathbf{v}
\end{bmatrix}
$$

显然对于这种问题直接求解线性系统更加稳定和高效。类似地，对于二次型的优化问题也建议使用构造线性系统的方式来进行求解。

<div align=center>
<img src="https://i.imgur.com/3njEYVU.png" width="80%">
</div>

### Mesh Embedding

在后面的课程里我们还会介绍**网格嵌入(mesh embedding)**的问题，它的目标是把一个三维空间中的网格展开为一个二维平面。

<div align=center>
<img src="https://i.imgur.com/xjfMPJg.png" width="80%">
</div>

实际上这样的问题可以转换为一个带约束的二次型优化问题，我们可以通过前面介绍过的方法来进行求解。

<div align=center>
<img src="https://i.imgur.com/8YuaRaK.png" width="80%">
</div>

更进一步，边界点的位置对于这样的参数化问题起着重要的作用。当我们没有显式设置边界点的位置时则需要引入一些额外的边界条件防止解的退化。

<div align=center>
<img src="https://i.imgur.com/sdEIltO.png" width="80%">
<img src="https://i.imgur.com/99ss9jU.png" width="80%">
<img src="https://i.imgur.com/yDE2zOk.png" width="80%">
</div>

### Unconstrained Optimization

当优化问题没有各种约束时就得到了**无约束优化问题(unconstrained optimization)**。求解无约束优化问题的基本方法是梯度下降法，它在机器学习中有着非常广泛的应用。

<div align=center>
<img src="https://i.imgur.com/Y5amF1T.png" width="80%">
</div>

当然梯度下降法并不是求解无约束优化问题最快的方法。如果我们可以计算优化目标的Hessian矩阵则可以使用Newton法来进行加速。

<div align=center>
<img src="https://i.imgur.com/Eo5rMdR.png" width="80%">
</div>

我们后面会介绍的形状插值就是使用这种方法来进行求解的。

<div align=center>
<img src="https://i.imgur.com/aJFngMT.png" width="80%">
<img src="https://i.imgur.com/i4SneuH.png" width="80%">
</div>

对于这些优化问题也无需自己编写求解算法，直接调用现成的求解器即可。

<div align=center>
<img src="https://i.imgur.com/4Y5WkjG.png" width="80%">
</div>

### Lagrangian Multiplier

## Reference

- [Lecture 02：Linear and variational problems](https://www.youtube.com/watch?v=oNbscH7sLc0&list=PLQ3UicqQtfNtUcdTMLgKSTTOiEsCw2VBW&index=2&ab_channel=JustinSolomon)