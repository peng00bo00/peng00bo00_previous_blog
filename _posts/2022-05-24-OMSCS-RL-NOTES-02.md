---
layout: article
title: OMSCS-RL课程笔记02-Reinforcement Learning Basics
tags: ["OMSCS", "CS7642-RL"]
key: OMSCS-RL-02
aside:
  toc: true
sidebar:
  nav: OMSCS-RL
---

> 这个系列是Gatech OMSCS 强化学习课程([CS 7642: Reinforcement Learning](https://omscs.gatech.edu/cs-7642-reinforcement-learning))的同步课程笔记。课程内容涉及强化学习算法的理论和相关应用，本节继续强化学习的概念。
<!--more-->

## Reinforcement Learning Basics

在上一节课中我们介绍过[MDP](/2022/05/21/OMSCS-RL-NOTES-01.html#markov-decision-process)的相关概念。MDP与我们通常意义下的强化学习还是有一些区别的，其主要区别在于MDP中模型的状态转移是已知而且固定的，而在强化学习中我们认为智能体对环境是一无所知的。这一区别造就了强化学习算法会使用不同的方法来估计环境的模型，或是完全脱离具体的环境直接进行学习。前者称为**基于模型的强化学习(model-based RL)**，而后者则称为**无模型强化学习(model-free RL)**。

## Behavior Structures

对于智能体而言，我们的目标是通过它和环境的互动来学习到一个决策函数。根据这个决策函数的不同行为，我们可以将它分为**规划(planning)**以及**条件规划(conditional planning)**：规划是指智能体会执行一系列确定的行为，这种算法在确定性的环境中往往有非常好的效果；而条件规划则是在规划中引入判断语句，这样智能体就可以对随机变化的环境进行处理。在MDP中我们可以把智能体的决策函数定义为一个关于智能体状态的函数，此时的决策函数称为stationary policy或者universal plan。这种定义方式的好处在于我们可以同时处理确定性和随机的环境，而且可以证明在MDP中一定存在一个最优的函数进行决策，即所谓的最优策略。

## Evaluating a Policy

我们在上一节课中提到过最优策略是指具有最大长期回报的策略。对于强化学习问题，由于环境对于智能体是未知的，我们需要通过一系列行为和反馈的样本来估计不同策略的回报。具体来说，我们首先需要知道智能体每条轨迹上环境的即时反馈$r$；对于长序列还需要进行一些截断从而获得一系列固定长度的轨迹样本；在这些样本的基础上就可以估计每条轨迹的回报$\sum_{i=1}^T \gamma^i r_i$；对于从同一初始状态出发的不同轨迹还可以通过取平均的方式来估计当前策略下该初始状态的价值。基于这种方式就可以估计给定策略下任意状态的价值，因此这一过程也称为**policy evaluation**。

## Evaluating a Learner

各种强化学习算法的目标是通过对智能体的轨迹进行采样来计算出当前环境下的最优策略。那如何来评价不同的学习算法呢？最直接的评价方法是计算各种算法输出策略的长期回报值，回报高的对应更好的学习算法；当然我们也需要考虑算法的复杂度，好的学习算法应该具有可以接受的计算复杂度；除此之外强化学习还有**experience complexity**的概念，它用来描述学习算法利用轨迹样本的效率，显然好的学习算法不需要过多的样本就能学到最优策略。

在后面的课程中我们会开始介绍各种不同的学习算法，而上面介绍的这些评价指标就是我们对不同学习算法进行比较的主要考量因素。