---
layout: article
title: GAMES001课程笔记01-线性代数基础
tags: ["CG", "GAMES001", "Math"]
key: GAMES001-01
aside:
  toc: true
sidebar:
  nav: GAMES001
---

> 这个系列是GAMES001-图形学中的数学([GAMES 001: Mathematics in Computer Graphics](https://games-cn.org/games001/))的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或"手册"，也可以作为站在不一样的视角复习图形学知识的平台。
<!--more-->

## 向量

**向量(vector)**是图形学中最基本的研究对象之一，像是空间中点的位置或是物体表面的法向都可以用向量来表示。实际上，早在中学阶段我们就接触过向量的概念。当时我们把向量定义为既有大小又有方向的量，一般用一个箭头来表示，同时向量之间的运算需要满足平行四边形法则。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/iYmk8gi.png" width="100%">
</div>

### 向量空间

关于向量更加严格的定义需要引入**向量空间(vector space)**的概念。具体来说，数域$$F$$上具有加法和标量乘法的非空集合$$V$$即为$$F$$上的向量空间，其中的元素则称为向量。一个向量空间需要满足一系列的性质，包括加法封闭性、标量乘法封闭性、加法交换律、加法结合律、标量乘法结合律、分配律等。一般来说图形学相关的研究中数域$$F$$大多是取为实数域$$\mathbb{R}$$，但有些时候也会取复数域$$\mathbb{C}$$或其他合适的数域。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/o28KYXr.png" width="100%">
</div>

### 线性组合

在向量空间的基础上我们可以引入**线性组合(linear combination)**的概念。简单来说，线性组合就是把一组向量$$\{ \mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n \}$$通过系数$$a_1$$, $$a_2$$, ..., $$a_n$$加起来。如果存在不全为0的一组系数能够使得$$a_1 \mathbf{u}_1 + a_2 \mathbf{u}_2 + ... + a_n \mathbf{u}_n = \mathbf{0}$$，则称向量集合$$\{ \mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n \}$$是**线性相关**的；反之则称这一组向量是**线性无关**的。

线性组合的一个重要应用是用来定义向量空间的**维度(dimension)**。我们把向量空间中能够找出的一组线性无关向量的个数的最大值称为向量空间的维度，记为$$\dim V$$。同时，$$\dim V$$个线性无关的向量可以构成向量空间中的一组**基矢(basis vectors)**。容易证明，空间中的任意一个向量都可以**唯一**地表示为这组基矢的线性组合，而线性组合的系数则称为该向量在这组基下的**坐标(coordinates)**。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/uCnc4k1.png" width="100%">
</div>

在图形学的研究中我们往往会接触到各种低维或高维的向量空间。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/VEMYsaH.png" width="100%">
</div>

## 矩阵

### 线性映射

两个向量空间$$V$$和$$W$$可以通过**线性映射(linear mapping)**$$f: V \rightarrow W$$进行联系，它需要满足如下定义：

- $$f(\mathbf{u} + \mathbf{v}) = f(\mathbf{u}) + f(\mathbf{v})$$
- $$f(\alpha \ \mathrm{v}) = \alpha f(\mathrm{v})$$

在低维空间中的缩放和旋转都是线性映射，但是需要注意平移**不是**线性映射。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Prx0gDa.png" width="100%">
</div>

### 矩阵

在线性映射的基础上我们可以定义**矩阵(matrix)**。尽管它最常见的形式是一个二维的数组，但它更合理的理解方式则是对线性映射的一种表示方法。实际上矩阵乘法的意义是给出原始空间$$V$$中的基矢在新空间$$W$$基矢下的坐标。在这种观点下，矩阵的乘法可以理解为线性映射的复合，即对空间多次相继变换的合成。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/hYztP3P.png" width="100%">
</div>

### 矩阵单目运算

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/LYHmkPB.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/wpVGDCZ.png" width="100%">
</div>

### 特征值

如果向量$$\mathbf{u}$$满足方程$$\mathbf{A} \mathbf{u} = \lambda \mathbf{u}$$，则称向量$$\mathbf{u}$$为矩阵$$\mathbf{A}$$的**特征向量(eigenvector)**，$$\lambda$$称为对应的**特征值(eigenvalue)**。从线性映射的角度来看，特征值的意义在于它说明了对于某些向量，线性映射的作用等价于用特征值进行数乘运算。

矩阵特征值的一个重要应用是用于计算矩阵多项式的特征值。对于矩阵$$\mathbf{A}$$，我们定义它的矩阵多项式为$$f(\mathbf{A}) = c_k \mathbf{A}^k + c_{k-1} \mathbf{A}^{k-1} + ... + c_1 \mathbf{A} + c_0 \mathbf{E}$$，则我们有如下结论：

- 若$$\lambda$$是矩阵$$\mathbf{A}$$的特征值，则$$f(\lambda)$$必为$$f(\mathbf{A})$$的特征值
- 设矩阵$$\mathbf{A}$$的特征值为$$\{ \lambda_1, \lambda_2, ..., \lambda_n \}$$；则$$f(\mathbf{A})$$的全部特征值由$$f(\lambda_1)$$, $$f(\lambda_2)$$, ..., $$f(\lambda_n)$$给出

上述性质表明矩阵多项式的特征值可以由矩阵的特征值来完全确定，这样我们就可以通过分析矩阵的特征值来理解矩阵多项式的行为。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/cZ9bqP0.png" width="100%">
</div>

### 度量空间

需要注意的是向量空间中的元素本身是不能进行比较的。如果要比较不同的元素则还需要定义一个度量函数$$d: V \times V \rightarrow \mathbb{R}$$，它需要满足非负性、同一性、对称性以及三角不等式等性质。定义了度量函数的集合$$V$$称为**度量空间(metric space)**，其中度量函数$$d$$描述了空间中元素之间的"距离"。

当我们在向量空间上引入度量函数就得到了**赋范向量空间(normed vector space)**，其度量函数$$d$$一般也被称为"范数"，记为$$\Vert \cdot \Vert$$。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/VVvj8PT.png" width="100%">
</div>

### 内积空间

在赋范向量空间的基础上我们还可以进一步定义**内积空间(inner product space)**，其中内积函数$$< \cdot, \cdot >: V \times V \rightarrow \mathbb{R}$$给出了向量之间的夹角。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/n51ykCb.png" width="100%">
</div>

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/r6UWE7W.png" width="100%">
</div>

## Reference

- [Lecture01: 线性代数基础](https://www.bilibili.com/video/BV1MF4m1V7e3/?vd_source=7a2542c6c909b3ee1fab551277360826)