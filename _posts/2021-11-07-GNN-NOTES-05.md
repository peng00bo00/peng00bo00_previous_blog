---
layout: article
title: 深蓝学院-GNN课程笔记05-图神经网络的鲁棒性
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-05
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图神经网络的对抗攻击和防御。
<!--more-->

传统深度神经网络在面对专门设计的**对抗攻击(adversarial attack)**时会出现鲁棒性较差的问题。以图像识别为例，我们只需要对输入图像进行微小的扰动即可完全改变图像识别的结果。

<div align=center>
<img src="https://i.imgur.com/uSEwGvR.png" width="70%">
</div>

而在图神经网络中也存在类似的问题。攻击者可以通过修改图结构或是节点属性来欺骗图神经网络，从而产生图对抗扰动。

<div align=center>
<img src="https://i.imgur.com/GnFtH9B.png" width="70%">
</div>

图神经网络的对抗攻击和防御在金融风控等领域中有着重要的研究价值，因此收到了人们越来越多的关注。

## 图对抗攻击

图对抗攻击的基本思路是对图进行一个微小扰动从而降低模型的性能。所谓"微小扰动"包括对图结构的扰动$\Delta A$以及对节点特征的扰动$\Delta F$，一般要求这些扰动小于某个阈值$\Delta$：

<div align=center>
<img src="https://i.imgur.com/GW9ywFO.png" width="70%">
</div>

常见的扰动形式包括在图上加边、减边、加节点或是修改节点特征等：

<div align=center>
<img src="https://i.imgur.com/MUfPJlv.png" width="70%">
</div>

### 图对抗攻击的类型

根据攻击者的能力，图对抗攻击可以分为**逃逸攻击(evasion attack)**和**投毒攻击(data poisoning attack)**两种：

- 逃逸攻击是指对训练好的模型进行攻击，此时攻击者不能修改模型的参数或是结构；
- 投毒攻击是指在模型训练前进行攻击，此时攻击者可以在训练数据中插入"毒药"从而干扰模型的训练过程。

<div align=center>
<img src="https://i.imgur.com/COaR1PY.png" width="40%">
<img src="https://i.imgur.com/fCpDSUU.png" width="43%">
</div>

而根据攻击者对模型的知识，图对抗攻击也可以分为**白盒攻击(white-box attack)**、**灰盒攻击(grey-box attack)**和**黑盒攻击(black-box attack)**：

- 白盒攻击是指攻击者可以获得被攻击的模型的完整信息，包括网络结构、模型参数和训练数据等；
- 灰盒攻击中攻击者不能访问模型的结构或参数，但可以获得被攻击的模型的训练数据；
- 黑盒攻击中攻击者不能访问任何与模型有关的信息，只允许从目标模型中进行查询来获得结果。

<div align=center>
<img src="https://i.imgur.com/Q8ctael.png" width="38%">
<img src="https://i.imgur.com/40j5tLJ.png" width="28%">
<img src="https://i.imgur.com/e4sqvLU.png" width="32%">
</div>

接下来我们会着重介绍这三种类型的常见攻击方法。

### 白盒攻击

#### PGD拓扑攻击

PGD拓扑攻击是通过修改图结构来进行攻击的方式，我们希望通过对图结构进行扰动来修改目标节点$v_i$的类别$y_i$。在讨论具体算法之前，首先需要介绍如何来表达对图结构进行扰动。扰动$\Delta A$可以表示为：

$$
\Delta A = (\bar A - A) \odot S
$$

$$
\bar A_{ij} = 
\left\{
\begin{aligned}
& 0, & & A_{ij} = 1 \\
& 1, & & A_{ij} = 0
\end{aligned}
\right.
$$

其中$\bar A$为邻接矩阵$A$的补；$\bar A - A$表示对图进行增边或是减边；$S$为对称布尔矩阵$S \in \{ 0, 1 \}^{N \times N}$表示攻击者修改了哪些边；$\odot$为矩阵的Hadamand乘积。

PGD拓扑攻击的基本思想就是通过学习对称布尔矩阵$S$来实现修改节点$v_i$的类别。对于给定的$S$矩阵和参数固定的GNN我们可以正向计算节点$v_i$的分类概率$z$。记$v_i$真实的标签为$y_i$，我们希望扰动后类别$y_i$的概率不再是最大值，因此可以构造损失函数：

$$
l(v_i) = z[y_i] - \max_{c \neq y_i} z[c]
$$

其中$$\max_{c \neq y_i} z[c]$$表示节点$v_i$属于其它类别概率的最大值。在很多情况下我们还希望这两者之间的概率差大于一定的阈值，因此可以构造一个改进的损失函数：

$$
l(v_i) = \max \big\{ z[y_i] - \max_{c \neq y_i} z[c] , -\kappa \big\}
$$

其中常数$\kappa > 0$。得到损失函数后就可以利用反向传播和梯度下降来更新布尔矩阵$S$，但需要说明的是更新后的$S$无法保证仍是布尔矩阵，因此需要适当进行放松将布尔矩阵松弛为介于[0, 1]之间的矩阵$S' \in [0, 1]^{N \times N}$并进行采样才能获得实际的扰动矩阵。PGD拓扑攻击的基本流程可参见下图：

<div align=center>
<img src="https://i.imgur.com/ryOVxIa.png" width="70%">
</div>

<div align=center>
<img src="https://i.imgur.com/yarQbLg.png" width="70%">
</div>

#### 基于积分梯度的攻击

我们还可以利用梯度信息作为分数来指导攻击。假设节点特征为离散特征，此时攻击者对图结构和节点信息的扰动都限制在0-1的离散值。因此我们可以利用梯度上升来最大化节点$v_i$的交叉熵来降低模型性能。但需要注意的是此时图结构和节点特征都是离散的，我们无法直接应用基于梯度的方法来得到攻击样本。

注意到损失函数对于邻接矩阵$A$的导数$S(i, j) = \frac{\partial l(v)}{\partial A_{ij}}$表达了在邻接矩阵进行微小扰动时损失函数的变化，因此我们可以把偏导数在[0, 1]区间上不同位置的取值相加(类似于积分)作为指示函数，它表示增加边$A_{ij}$所导致交叉熵损失函数的增加。这样可以取$S(i, j)$中最大的进行改变即完成了对图结构的修改。而对于减边的情况则只需要将相加的和取反即可。

综合以上，可以得到基于积分梯度的对抗攻击流程如下图所示：

<div align=center>
<img src="https://i.imgur.com/UUlBlWX.png" width="70%">
</div>

### 灰盒攻击

在灰盒攻击中，攻击者不仅仅鞥访问受害者模型的结构和参数，但可以访问用于训练的数据。因此灰盒攻击通常不是直接攻击给定的模型，而是首先利用训练数据训练一个代理模型，然后攻击这个代理模型。

#### Nettack

Nettack的目标是生成针对节点分类任务的对抗图。对于攻击目标$v_i$以及它的类别$y_i$，Nettack试图修改图的邻接矩阵和节点特征使得GNN将$v_i$分类为其它的标签，同时修改后对邻接矩阵以及节点特征的扰动要尽可能小：

$$
\Vert A' - A \Vert_0 + \Vert F' - F \Vert_0 \leq \Delta
$$

除此之外，Nettack还要求新图的度的分布以及特征共现与原图要尽可能接近。

和白盒攻击相比，Nettack首先需要训练一个代理模型GNN$_\text{sur}$，在原始论文中将这个代理模型设置为没有激活函数的2层GCN模型。然后Nettack开始攻击这个代理模型，类似于PGD拓扑攻击在Nettack中的目标函数为：

$$
l(v_i) = \max_{c \neq y_i} z[c] - z[y_i]
$$

注意此时是最大化目标函数。然后Nettack会选择使$l(v_i)$变化最大的一条边并进行加边减边扰动。Nettack的主要流程可参见下图：

<div align=center>
<img src="https://i.imgur.com/oyTi5YV.png" width="65%">
</div>

#### Meta Attack

Meta Attack同样是一种针对节点分类任务的灰盒攻击攻击方法。与Nettack不同的是，在Meta Attack中攻击者希望降低GNN在测试集上的整体分类性能。需要注意的另一点是Meta Attack是一种投毒攻击，受害模型需要在污染过的图上进行训练和测试。

Meta Attack首先需要训练代理模型GNN$_\text{sur}$，但与Nettack不同的地方在于代理模型需要分别在原始数据以及污染后的数据上进行训练。Meta Attack的攻击目标是使同样结构的模型在扰动前后的数据上有较大的差异，即最大化节点类别的交叉熵：

$$
L_\text{self} = L(f_\text{GNN}(A'; \Theta^*), C_u')
$$

其中$C_u'$是代理模型在原始数据上对节点的预测标签。在原始论文中还添加了代理模型在污染数据集上的交叉熵，从而得到最终的损失函数：

$$
L_\text{atk} (A') = -L_\text{self} - L_\text{tr} (A')
$$

整理一下，Meta Attack的本质是求解如下定义的双层优化问题：

$$
\begin{aligned}
\min_{A'} \ \ & L_\text{atk} (f_\text{GNN}(A'; \Theta^*)) \\
\text{s.t.} \ \ & \Theta^* = \arg \min_\Theta L_\text{tr} (f_\text{GNN}(A'; \Theta))
\end{aligned}
$$

<div align=center>
<img src="https://i.imgur.com/ocuWeo5.png" width="70%">
</div>

### 黑盒攻击

## 图对抗防御