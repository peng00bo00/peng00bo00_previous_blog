---
layout: article
title: 深蓝学院-GNN课程笔记05-图神经网络的鲁棒性
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-05
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图神经网络的对抗攻击和防御。
<!--more-->

传统深度神经网络在面对专门设计的**对抗攻击(adversarial attack)**时会出现鲁棒性较差的问题。以图像识别为例，我们只需要对输入图像进行微小的扰动即可完全改变图像识别的结果。

<div align=center>
<img src="https://i.imgur.com/uSEwGvR.png" width="70%">
</div>

而在图神经网络中也存在类似的问题。攻击者可以通过操纵图结构或是节点属性来欺骗图神经网络，从而生成图对抗扰动。

<div align=center>
<img src="https://i.imgur.com/GnFtH9B.png" width="70%">
</div>

图神经网络的对抗攻击和防御在金融风控等领域中有着重要的研究价值，因此收到了人们越来越多的关注。

## 图对抗攻击

图对抗攻击的基本思路是对图进行一个微小扰动从而降低模型的性能。所谓"微小扰动"包括对图结构的扰动$\Delta A$以及对节点特征的扰动$\Delta F$，一般要求这些扰动小于某个阈值$\Delta$：

<div align=center>
<img src="https://i.imgur.com/GW9ywFO.png" width="70%">
</div>

常见的扰动形式包括在图上加边、减边、加节点或是修改节点特征等：

<div align=center>
<img src="https://i.imgur.com/MUfPJlv.png" width="70%">
</div>

### 图对抗攻击的类型

根据攻击者的能力，图对抗攻击可以分为**逃逸攻击(evasion attacks)**和**投毒攻击(data poisoning attack)**两种：

- 逃逸攻击是指对训练好的模型进行攻击，此时攻击者不能修改模型的参数或是结构；
- 投毒攻击是指在模型训练前进行攻击，此时攻击者可以在训练数据中插入"毒药"从而干扰模型的训练过程。

<div align=center>
<img src="https://i.imgur.com/COaR1PY.png" width="40%">
<img src="https://i.imgur.com/fCpDSUU.png" width="43%">
</div>

而根据攻击者对模型的知识，图对抗攻击也可以分为**白盒攻击(white-box attack)**、**灰盒攻击(grey-box attack)**和**黑盒攻击(black-box attack)**：

- 白盒攻击是指攻击者可以获得被攻击的模型的完整信息，包括网络结构、模型参数和训练数据等；
- 灰盒攻击中攻击者不能访问模型的结构或参数，但可以获得被攻击的模型的训练数据；
- 黑盒攻击中攻击者不能访问任何与模型有关的信息，只允许从目标模型中进行查询来获得结果。

<div align=center>
<img src="https://i.imgur.com/Q8ctael.png" width="38%">
<img src="https://i.imgur.com/40j5tLJ.png" width="28%">
<img src="https://i.imgur.com/e4sqvLU.png" width="32%">
</div>

接下来我们会着重介绍这三种类型的常见攻击方法。

### 白盒攻击

### 灰盒攻击

### 黑盒攻击

## 图对抗防御