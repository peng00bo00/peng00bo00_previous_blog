---
layout: article
title: 深蓝学院-GNN课程笔记05-图神经网络的鲁棒性
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-05
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图神经网络的对抗攻击和防御。
<!--more-->

传统深度神经网络在面对专门设计的**对抗攻击(adversarial attack)**时会出现鲁棒性较差的问题。以图像识别为例，我们只需要对输入图像进行微小的扰动即可完全改变图像识别的结果。

<div align=center>
<img src="https://i.imgur.com/uSEwGvR.png" width="70%">
</div>

而在图神经网络中也存在类似的问题。攻击者可以通过操纵图结构或是节点属性来欺骗图神经网络，从而生成图对抗扰动。

<div align=center>
<img src="https://i.imgur.com/GnFtH9B.png" width="70%">
</div>

图神经网络的对抗攻击和防御在金融风控等领域中有着重要的研究价值，因此收到了人们越来越多的关注。

## 图对抗攻击

图对抗攻击的基本思路是对图进行一个微小扰动从而降低模型的性能。所谓"微小扰动"包括对图结构的扰动$\Delta A$以及对节点特征的扰动$\Delta F$，一般要求这些扰动小于某个阈值$\Delta$：

<div align=center>
<img src="https://i.imgur.com/GW9ywFO.png" width="70%">
</div>

常见的扰动形式包括在图上加边、减边、加节点或是修改节点特征等：

<div align=center>
<img src="https://i.imgur.com/MUfPJlv.png" width="70%">
</div>

### 图对抗攻击的类型

根据攻击者的能力，图对抗攻击可以分为**逃逸攻击(evasion attacks)**和**投毒攻击(data poisoning attack)**两种：

- 逃逸攻击是指对训练好的模型进行攻击，此时攻击者不能修改模型的参数或是结构；
- 投毒攻击是指在模型训练前进行攻击，此时攻击者可以在训练数据中插入"毒药"从而干扰模型的训练过程。

<div align=center>
<img src="https://i.imgur.com/COaR1PY.png" width="40%">
<img src="https://i.imgur.com/fCpDSUU.png" width="43%">
</div>

而根据攻击者对模型的知识，图对抗攻击也可以分为**白盒攻击(white-box attack)**、**灰盒攻击(grey-box attack)**和**黑盒攻击(black-box attack)**：

- 白盒攻击是指攻击者可以获得被攻击的模型的完整信息，包括网络结构、模型参数和训练数据等；
- 灰盒攻击中攻击者不能访问模型的结构或参数，但可以获得被攻击的模型的训练数据；
- 黑盒攻击中攻击者不能访问任何与模型有关的信息，只允许从目标模型中进行查询来获得结果。

<div align=center>
<img src="https://i.imgur.com/Q8ctael.png" width="38%">
<img src="https://i.imgur.com/40j5tLJ.png" width="28%">
<img src="https://i.imgur.com/e4sqvLU.png" width="32%">
</div>

接下来我们会着重介绍这三种类型的常见攻击方法。

### 白盒攻击

#### PGD拓扑攻击

PGD拓扑攻击是通过修改图结构来进行攻击的方式，我们希望通过对图结构进行扰动来修改目标节点$v_i$的类别$y_i$。在讨论具体算法之前，首先需要介绍如何来表达对图结构进行扰动。扰动$\Delta A$可以表示为：

$$
\Delta A = (\bar A - A) \odot S
$$

$$
\bar A_{ij} = 
\left\{
\begin{aligned}
& 0, & & A_{ij} = 1 \\
& 1, & & A_{ij} = 0
\end{aligned}
\right.
$$

其中$\bar A$为邻接矩阵$A$的补；$\bar A - A$表示对图进行增边或是减边；$S$为对称布尔矩阵$S \in \{ 0, 1 \}^{N \times N}$表示攻击者修改了哪些边；$\odot$为矩阵的Hadamand乘积。

PGD拓扑攻击的基本思想就是通过学习对称布尔矩阵$S$来实现修改节点$v_i$的类别。对于给定的$S$矩阵和参数固定的GNN我们可以正向计算节点$v_i$的分类概率$z$。记$v_i$真实的标签为$y_i$，我们希望扰动后类别$y_i$的概率不再是最大值，因此可以构造损失函数：

$$
l(v_i) = z[y_i] - \max_{c \neq y_i} z[c]
$$

其中$$\max_{c \neq y_i} z[c]$$表示节点$v_i$属于其它类别概率的最大值。在很多情况下我们还希望这两者之间的概率差大于一定的阈值，因此可以构造一个改进的损失函数：

$$
l(v_i) = \max \big\{ z[y_i] - \max_{c \neq y_i} z[c] , -\kappa \big\}
$$

其中常数$\kappa > 0$。得到损失函数后就可以利用反向传播和梯度下降来更新布尔矩阵$S$，但需要说明的是更新后的$S$无法保证仍是布尔矩阵，因此需要适当进行放松将布尔矩阵松弛为介于[0, 1]之间的矩阵$S' \in [0, 1]^{N \times N}$并进行采样才能获得实际的扰动矩阵。PGD拓扑攻击的基本流程可参见下图：

<div align=center>
<img src="https://i.imgur.com/ryOVxIa.png" width="70%">
</div>

<div align=center>
<img src="https://i.imgur.com/yarQbLg.png" width="70%">
</div>

#### 基于积分梯度的攻击

我们还可以利用梯度信息作为分数来指导攻击。假设节点特征为离散特征，此时攻击者对图结构和节点信息的扰动都限制在0-1的离散值。因此我们可以利用梯度上升来最大化节点$v_i$的交叉熵来降低模型性能。但需要注意的是此时图结构和节点特征都是离散的，我们无法直接应用基于梯度的方法来得到攻击样本。

注意到损失函数对于邻接矩阵$A$的导数$S(i, j) = \frac{\partial l(v)}{\partial A_{ij}}$表达了在邻接矩阵进行微小扰动时损失函数的变化，因此我们可以把偏导数在[0, 1]区间上不同位置的取值相加(类似于积分)作为指示函数，它表示增加边$A_{ij}$所导致交叉熵损失函数的增加。这样可以取$S(i, j)$中最大的进行改变即完成了对图结构的修改。而对于减边的情况则只需要将相加的和取反即可。

综合以上，可以得到基于积分梯度的对抗攻击流程如下图所示：

<div align=center>
<img src="https://i.imgur.com/UUlBlWX.png" width="70%">
</div>

### 灰盒攻击

### 黑盒攻击

## 图对抗防御