---
layout: article
title: Shape Analysis课程笔记16-Vector Fields Discretization
tags: ["CG", "Geometry Processing", "Shape Analysis"]
key: SA-16
aside:
  toc: true
sidebar:
  nav: SA
---

> 这个系列是[MIT 6.838: Shape Analysis](https://groups.csail.mit.edu/gdpgroup/6838_spring_2021.html)的同步课程笔记。本课程会介绍几何方法在图形学、机器学习、计算机视觉、医疗图像以及建筑设计等相关领域的原理和应用。本节主要介绍向量场在三角网格上的应用。
<!--more-->

目前向量场在三角网格上的表示方法主要有3种：基于面的、基于边的以及基于顶点的表示方法。

<div align=center>
<img src="https://i.imgur.com/D7t2jhd.png" width="80%">
</div>

## Triangle-Based

离散向量场在表示时的难点在于三角网格上无法定义严格的切平面。而在基于面的表示方法中，我们直接把每个三角形所在平面当做曲面的切平面，这样就只需要为每一个面赋予平面内的一个向量就得到了向量场。

<div align=center>
<img src="https://i.imgur.com/KW3nAFy.png" width="80%">
</div>

### Discrete Levi-Civita Connection

基于面的方法还有一个优势在于我们可以很容易地定义平行传输，此时只需要把相邻三角形展开成平面然后直接移动向量即可。但需要注意的是由于Gauss曲率的存在，在同一顶点连接的三角形上经过平行传输一圈后向量会和初始向量存在一个夹角。

<div align=center>
<img src="https://i.imgur.com/qgmwnaN.png" width="80%">
</div>

为了处理这样的问题，我们可以在进行平行传输的时候额外旋转一个角度。即为三角网格的每一条边赋予一个平面旋转，当平行传输经过这条边时需要再进行相应的旋转。

<div align=center>
<img src="https://i.imgur.com/7Q83gQM.png" width="80%">
</div>

### Trivial Connection

这样就可以保证向量在进行平行传输时不会有holonomy的现象，同时还可以证明如此构造的向量场是与传输路径无关的。

<div align=center>
<img src="https://i.imgur.com/jevZoFQ.png" width="80%">
</div>

基于这种思路我们可以把向量场的设计转换为每条边旋转角度的计算，进而构造一个约束优化问题。

<div align=center>
<img src="https://i.imgur.com/WviEl2a.png" width="80%">
</div>

<div align=center>
<img src="https://i.imgur.com/KCUowhe.png" width="80%">
</div>

### Helmholtz-Hodge Decomposition

在面上设计向量场的另一个强大工具是**Helmholtz-Hodge分解(Helmholtz-Hodge decomposition)**，它指出任意向量场可以表示为无散场、无旋场以及调和场之和。

<div align=center>
<img src="https://i.imgur.com/k9Sadz9.png" width="80%">
</div>

结合维度分析可以得到三个场的维度分别为$\vert E \vert - 1$、$\vert V \vert - 1$和$2g$。

<div align=center>
<img src="https://i.imgur.com/MQRfSsV.png" width="80%">
</div>

实际上无散场和无旋场的维度还可以互换。

<div align=center>
<img src="https://i.imgur.com/E8uItB7.png" width="80%">
</div>

不过需要注意的是对于边上定义的场可能会出现不连续的情况。

<div align=center>
<img src="https://i.imgur.com/gX08hic.png" width="80%">
</div>

<div align=center>
<img src="https://i.imgur.com/OS1LZrS.png" width="80%">
</div>

## Edge-Based

<div align=center>
<img src="https://i.imgur.com/b41SeS1.png" width="80%">
</div>

## Vertex-Based

最后一种常见的表示离散向量场的方式是基于顶点的表示。这种方式的好处在于允许我们计算高阶的微分，但是需要注意在顶点上没有天然的切平面，而且也要考虑Gauss曲率的影响。

<div align=center>
<img src="https://i.imgur.com/L0E1Jbr.png" width="80%">
</div>

## 2D Planar Case

对于二维平面的情况，基于顶点的向量场表达非常容易。我们只需要为每一个顶点赋予一个平面向量，然后在三角形内对顶点进行插值即可。

<div align=center>
<img src="https://i.imgur.com/64aJdcG.png" width="80%">
</div>

平面图形的运动变形就可以基于这样的二维向量场来实现。

<div align=center>
<img src="https://i.imgur.com/RFQv4Pg.png" width="80%">
</div>

### 3D Case

在三维空间中的三角网格上则不能直接通过插值的方式来获得切向量。

<div align=center>
<img src="https://i.imgur.com/HWiSWfw.png" width="80%">
</div>

在这种情况下可以考虑geodesic polar map来进行插值。

<div align=center>
<img src="https://i.imgur.com/K0Kh7dV.png" width="80%">
</div>

<div align=center>
<img src="https://i.imgur.com/oIsmOIO.png" width="80%">
</div>

## Operator-Based

除了上面介绍过的几种方式，近几年还出现了一些基于算子的向量场表示方法。

<div align=center>
<img src="https://i.imgur.com/seuifto.png" width="80%">
</div>

<div align=center>
<img src="https://i.imgur.com/eorkXVR.png" width="80%">
</div>

## Continuous Normalizing Flows

向量场设计出了CG领域外在机器学习中也有着相等多的应用。**normalizing flow(NF)**是近几年出现的一种生成式模型，更多的技术细节可以参考论文[Normalizing Flows for Probabilistic Modeling and Inference](https://jmlr.org/papers/volume22/19-1028/19-1028.pdf)。

### Problem Description

NF的目标是对复杂的概率分布进行建模。这里需要说明的是现实生活中的数据往往都来自于非常复杂的概率分布，我们很难只用少量的参数来表达它们。

<div align=center>
<img src="https://i.imgur.com/BasVzKH.png" width="80%">
</div>

而NF的思路是从相对简单的**basic distribution** $u \sim p_\psi(u)$进行采样，通过设计一个映射$T_\phi$来获得所需的概率分布。

<div align=center>
<img src="https://i.imgur.com/1nN6hMj.png" width="80%">
</div>

### Sampling

为了从$p_\psi$中进行采样，我们一般要求已知$p_\psi$的pdf而且有比较简单的采样算法。在大多数情况下我们可以把(标准)正态分布作为basic distribution。

<div align=center>
<img src="https://i.imgur.com/NB5VjGY.png" width="80%">
</div>

### Mapping

NF的目标是学习一个basic distribution到$p_\phi$的映射，它一般可以由一个神经网络来表示。

<div align=center>
<img src="https://i.imgur.com/Pu5Yvdk.png" width="80%">
</div>

我们进一步要求映射$T_\phi$及其逆$T_\phi^{-1}$是可微的，则目标分布的概率密度可以表示为：

$$
p(x) = p_\psi (u) \vert \det{J_{T_\phi}(u)} \vert^{-1}, \ \text{where} \ u = T_\phi^{-1}(x)
$$

其中$J_{T_\phi}$为映射$T_\phi$的Jacobian矩阵。

<div align=center>
<img src="https://i.imgur.com/1ZlACCj.png" width="80%">
</div>

### Example

### Continuous Normalizing Flow

### Log-Density

## Reference

- [Lecture 17: Vector fields on triangle meshes](https://www.youtube.com/watch?v=Y3R-olnX_Lc&list=PLQ3UicqQtfNtUcdTMLgKSTTOiEsCw2VBW&index=23)
- [Lecture 17 (extra content): Continuous normalizing flows](https://www.youtube.com/watch?v=RthYBGDk7lQ&list=PLQ3UicqQtfNtUcdTMLgKSTTOiEsCw2VBW&index=24)
- [Vector Fields](https://groups.csail.mit.edu/gdpgroup/assets/6838_spring_2021/11_vector_fields.pdf)