---
layout: article
title: 深蓝学院-GNN课程笔记02-深度学习基础
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-02
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍深度学习的基础知识。
<!--more-->

## 深度学习简史

所谓深度学习(Deep Learning)是指使用深层的神经网络来解决机器学习的问题，其大致发展脉络如下：

<div align=center>
<img src="https://i.imgur.com/1LDcRqx.png" width="41%">
<img src="https://i.imgur.com/es61qfQ.png" width="40%">
</div>

早在上世纪四十年代研究人员就受到生物神经系统的启发提出了**人工神经元模型(McCulloch-Pitts neuron)**。在五十年代，人工神经网络不断发展并产生了**感知机模型(perceptron)**。到了六十年代，**反向传播(backpropagation)**的思想最早出现在控制理论中，随后由Hinton等拓展到神经网络的训练过程并一直沿用到今天。而在八十年代出现了早期的**卷积神经网络(convolutional neural network, CNN)**和**循环神经网络(recurrent neural network, RNN)**等模型，并在图像识别以及序列数据分析等领域得到了一定的应用。

<div align=center>
<img src="https://cdn-cfofm.nitrocdn.com/MMCsahxLgXapyDWrjVzGzdjiRyqxAjlx/assets/static/optimized/brainmadesimple.com/wp-content/uploads/2019/10/64c6868a7266d557c41b169b041221b1.neuronial-developement.png" width="40%">
<img src="https://i.imgur.com/Nk3pmbN.png" width="40%">
</div>

进入二十一世纪，随着数据量以及计算能力的增长，深度学习取得了爆发性的进步。高速GPU的出现使得训练大规模神经网络模型成为了可能，同时超大规模训练数据库的出现又保证了模型具有足够强的泛化能力。因此深度学习的相关方法在各种各样应用中的表现都远远高于传统的机器学习方法，在不同的研究领域中都取得了巨大的成功和社会影响力。在2012年的ILSVRC图像识别竞赛中，使用深度卷积网络的AlexNet将图像识别的错误率由25.8%降低到了16.4%。随后几年深度卷积网络迅速统治了图像识别领域并成功将图像识别的错误率降低到了3.5%，已经低于人眼的识别错误率。2016年基于**深度强化学习(deep reinforcement learning)**的AlphaGo模型首次在围棋领域击败了人类最强棋手，这一成绩使得人工智能的概念进入到大众视野。到了2018年，基于深度学习的预训练语言模型BERT横空出世并在**自然语言处理(natural language processing, NLP)**的各个领域中都取得了突破性的进步。同年，图灵奖颁发给了Yoshua Bengio，Geoffrey Hinton以及Lann Lecun以表彰他们在深度学习领域做出的奠基性工作。

<div align=center>
<img src="https://i.imgur.com/0Xs0t3M.png" width="70%">
</div>

## 前馈神经网络

### 网络结构

最简单的深度学习模型是**前馈神经网络(feedforward neural network)**，网络包括一个输入层、若干个隐层以及一个输出层，从而将一个输入向量映射为输出向量。

<div align=center>
<img src="https://i.imgur.com/m2b7sEt.png" width="50%">
</div>

在隐层中每个神经元接收来自前一层的输入信号并进行加权求和，然后通过一个非线性**激活函数(activation function)**将累计的信号进行激活从而得到传递给下一神经元的输出值。单个神经元的计算过程如下图所示。

<div align=center>
<img src="https://i.imgur.com/DXBfqGb.png" width="50%">
</div>

将每一个隐层的神经元放到一起可以得到矩阵形式的前向计算公式：

$$
h^{(k+1)} = f^{(k+1)} (h^{(k)}) = \alpha(b^{(k)} + W^{(k)} h^{(k)})
$$

其中$h^{(k)}$为第$k$个隐层的输出向量；$W^{(k)}$为第$k$个隐层的权重矩阵；$b^{(k)}$为第$k$个隐层的偏置向量。

### 激活函数

每个神经元通过非线性激活函数来增强它的表达能力，常见的激活函数包括ReLU、sigmoid以及tanh等。

$$
\text{ReLU} (x) = \max \{ x,  0\}
$$

<div align=center>
<img src="https://raw.githubusercontent.com/siebenrock/activation-functions/master/plots/rectified_linear_units.png" width="40%">
</div>

$$
\sigma (x) = \frac{1}{1 + e^{-x}}
$$

<div align=center>
<img src="https://raw.githubusercontent.com/siebenrock/activation-functions/master/plots/sigmoid.png" width="40%">
</div>

$$
\tanh (x) = \frac{2}{1 + e^{-2x}} - 1 = 2 \sigma (2x) - 1
$$

<div align=center>
<img src="https://raw.githubusercontent.com/siebenrock/activation-functions/master/plots/hyperbolic_tangent.png" width="40%">
</div>

值得一提的是sigmoid函数和tanh函数都存在一定的饱和问题。当输入$x$过大或者过小时，函数的梯度都趋于0；只有当输入$x$在0附近时两个激活函数才会比较敏感。因此现代神经网络中使用sigmoid和tanh函数的模型越来越少，大多数模型都会默认选择ReLU作为激活函数。

### 输出层和损失函数

神经网络的输出层和一般的隐层并没有本质的区别，仍然是对前一层进行加权求和。但需要主要的是大多数情况下输出层不包含激活函数。

$$
\hat{y} = W h + b
$$

<div align=center>
<img src="https://i.imgur.com/6Obgmwg.png" width="70%">
</div>

除了模型之外我们还需要**损失函数(loss function)**来度量网络的输出与ground truth的差异。对于回归问题一般可采用平方误差作为损失函数：

$$
l(y, \hat{y}) = (y - \hat{y})^2
$$

而对于分类问题则需要使用softmax函数将网络输出转换成类别向量，然后利用**交叉熵(cross entropy)**计算损失：

$$
z = W h + b
$$

$$
\hat{y}_i = \frac{\exp (z_i)}{\sum_j \exp (z_j)}
$$

$$
l(y, \hat{y}) = -\sum_i y_i \log ( \hat{y}_i )
$$

## 神经网络的训练

### 梯度下降

我们通过在训练数据集上最小化损失函数来实现神经网络的训练。显然神经网络的训练过程是一个优化问题，可以通过**梯度下降(gradient descent)**的方法进行求解：

$$
W_{n+1} \leftarrow W_{n} - \eta \frac{\partial l}{\partial W_n}
$$

<div align=center>
<img src="https://i.imgur.com/CKr3qCB.png" width="70%">
</div>

其中$W_n$为网络在当前步的参数；$\eta$为**学习率(learning rate)**，它控制了每次进行梯度下降的步长。

由于在整个数据集上计算导数十分消耗计算资源，在大多数情况下我们会每次在数据集上采样出一小部分数据来进行计算，此时梯度下降算法称为**小批量梯度下降(mini-batch gradient descent)**。除此之外，梯度下降还存在一些变体如Adagrad、Adadelta以及Adam算法等。这些变体使用了**动量项(momentum)**来修正梯度值，在很多情况下这些变体会有更好的收敛性。

### 反向传播

通过梯度下降训练神经网络首先需要知道损失函数对于模型参数的梯度，一般是通过反向传播算法来进行计算。反向传播算法的本质是利用**链式法则(chain rule)**从后向前逐层计算导数。对于可导的损失函数$l$首先计算它对于输出层的导数$\frac{\partial l}{\partial o}$，根据链式法则$l$对于最后一个隐层的导数为：

$$
\frac{\partial l}{\partial h^k} = \frac{\partial o}{\partial h^k} \cdot \frac{\partial l}{\partial o}
$$

$$
\frac{\partial l}{\partial w_{h^k}} = \frac{\partial o}{\partial w_{h^k}} \cdot \frac{\partial l}{\partial o}
$$

类似地，损失函数$l$对于第$r-1$层的导数为：

$$
\frac{\partial l}{\partial h^{r-1}} = \frac{\partial h^r}{\partial h^{r-1}} \cdot \frac{\partial l}{\partial h^r} = \frac{\partial h^r}{\partial h^{r-1}} \cdot \bigg( \frac{\partial h^{r+1}}{\partial h^r} \cdots \frac{\partial o}{\partial h^k} \cdot \frac{\partial l}{\partial o} \bigg)
$$

$$
\frac{\partial l}{\partial w_{h^{r-1}}} = \frac{\partial h^{r-1}}{\partial w_{h^{r-1}}} \cdot \frac{\partial l}{\partial h^{r-1}} = \frac{\partial h^{r-1}}{\partial w_{h^{r-1}}} \cdot \bigg( \frac{\partial h^r}{\partial h^{r-1}} \cdots \frac{\partial o}{\partial h^k} \cdot \frac{\partial l}{\partial o} \bigg)
$$

<div align=center>
<img src="https://i.imgur.com/yFEgEt3.png" width="90%">
</div>

其中$\frac{\partial l}{\partial h^{r-1}} = \frac{\partial h^r}{\partial h^{r-1}} \cdot \big( \frac{\partial h^{r+1}}{\partial h^r} \cdots \frac{\partial o}{\partial h^k} \cdot \frac{\partial l}{\partial o} \big)$可通过从后向前递推，因此每次只需要单独计算每一层的导数$\frac{\partial h^{r-1}}{\partial w_{h^{r-1}}}$即可。

### 预防过拟合

深度神经网络往往具有大量的模型参数因此容易出现各种过拟合问题。常用的预防过拟合技术包括加入**正则化项(regularization)**、**随机失活(dropout)**以及**批量归一化(batch normalization)**等。正则化项是指对模型的参数加以约束，防止模型参数出现过于大的值，一般可通过在损失函数中加入模型参数的L1或L2范数来实现。随机失活是指在模型训练过程中随机将一部分神经元的输出置为0，仅保留剩余神经元的计算结果并送入下一层中。通过随机失活可以约束模型的表达能力从而防止出现过拟合的现象。而批量归一化则是对隐层的输出向量进行标准化，将隐层的输出规范化到一定的范围上从而对模型的学习能力进行限制。

## 卷积神经网络

## 循环神经网络

## 自编码器