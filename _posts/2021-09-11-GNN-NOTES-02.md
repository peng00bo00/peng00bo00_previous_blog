---
layout: article
title: 深蓝学院-GNN课程笔记02-深度学习基础
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-02
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍深度学习的基础知识。
<!--more-->

## 深度学习简史

所谓深度学习(Deep Learning)是指使用深层的神经网络来解决机器学习的问题，其大致发展脉络如下：

<div align=center>
<img src="https://i.imgur.com/1LDcRqx.png" width="41%">
<img src="https://i.imgur.com/es61qfQ.png" width="40%">
</div>

早在上世纪四十年代研究人员就受到生物神经系统的启发提出了**人工神经元模型(McCulloch-Pitts neuron)**。在五十年代，人工神经网络不断发展并产生了**感知机模型(perceptron)**。到了六十年代，**反向传播(backpropagation)**的思想最早出现在控制理论中，随后由Hinton等拓展到神经网络的训练过程并一直沿用到今天。而在八十年代出现了早期的**卷积神经网络(convolutional neural network, CNN)**和**循环神经网络(recurrent neural network, RNN)**等模型，并在图像识别以及序列数据分析等领域得到了一定的应用。

<div align=center>
<img src="https://cdn-cfofm.nitrocdn.com/MMCsahxLgXapyDWrjVzGzdjiRyqxAjlx/assets/static/optimized/brainmadesimple.com/wp-content/uploads/2019/10/64c6868a7266d557c41b169b041221b1.neuronial-developement.png" width="40%">
<img src="https://i.imgur.com/Nk3pmbN.png" width="40%">
</div>

进入二十一世纪，随着数据量以及计算能力的增长，深度学习取得了爆发性的进步。高速GPU的出现使得训练大规模神经网络模型成为了可能，同时超大规模训练数据库的出现又保证了模型具有足够强的泛化能力。因此深度学习的相关方法在各种各样应用中的表现都远远高于传统的机器学习方法，在不同的研究领域中都取得了巨大的成功和社会影响力。在2012年的ILSVRC图像识别竞赛中，使用深度卷积网络的AlexNet将图像识别的错误率由25.8%降低到了16.4%。随后几年深度卷积网络迅速统治了图像识别领域并成功将图像识别的错误率降低到了3.5%，已经低于人眼的识别错误率。2016年基于**深度强化学习(deep reinforcement learning)**的AlphaGo模型首次在围棋领域击败了人类最强棋手，这一成绩使得人工智能的概念进入到大众视野。到了2018年，基于深度学习的预训练语言模型BERT横空出世并在**自然语言处理(natural language processing, NLP)**的各个领域中都取得了突破性的进步。同年，图灵奖颁发给了Yoshua Bengio，Geoffrey Hinton以及Lann Lecun以表彰他们在深度学习领域做出的奠基性工作。

<div align=center>
<img src="https://i.imgur.com/0Xs0t3M.png" width="70%">
</div>

## 前馈神经网络

### 网络结构

最简单的深度学习模型是**前馈神经网络(feedforward neural network)**，网络包括一个输入层、若干个隐层以及一个输出层，从而将一个输入向量映射为输出向量。

<div align=center>
<img src="https://i.imgur.com/m2b7sEt.png" width="50%">
</div>

在隐层中每个神经元接收来自前一层的输入信号并进行加权求和，然后通过一个非线性**激活函数(activation function)**将累计的信号进行激活从而得到传递给下一神经元的输出值。单个神经元的计算过程如下图所示。

<div align=center>
<img src="https://i.imgur.com/DXBfqGb.png" width="50%">
</div>

将每一个隐层的神经元放到一起可以得到矩阵形式的前向计算公式：

$$
h^{(k+1)} = f^{(k+1)} (h^{(k)}) = \alpha(b^{(k)} + W^{(k)} h^{(k)})
$$

其中$h^{(k)}$为第$k$个隐层的输出向量；$W^{(k)}$为第$k$个隐层的权重矩阵；$b^{(k)}$为第$k$个隐层的偏置向量。

### 激活函数

每个神经元通过非线性激活函数来增强它的表达能力，常见的激活函数包括ReLU、sigmoid以及tanh等。

$$
\text{ReLU} (x) = \max \{ x,  0\}
$$

<div align=center>
<img src="https://raw.githubusercontent.com/siebenrock/activation-functions/master/plots/rectified_linear_units.png" width="40%">
</div>

$$
\sigma (x) = \frac{1}{1 + e^{-x}}
$$

<div align=center>
<img src="https://raw.githubusercontent.com/siebenrock/activation-functions/master/plots/sigmoid.png" width="40%">
</div>

$$
\tanh (x) = \frac{2}{1 + e^{-2x}} - 1 = 2 \sigma (2x) - 1
$$

<div align=center>
<img src="https://raw.githubusercontent.com/siebenrock/activation-functions/master/plots/hyperbolic_tangent.png" width="40%">
</div>

值得一提的是sigmoid函数和tanh函数都存在一定的饱和问题。当输入$x$过大或者过小时，函数的梯度都趋于0；只有当输入$x$在0附近时两个激活函数才会比较敏感。因此现代神经网络中使用sigmoid和tanh函数的模型越来越少，大多数模型都会默认选择ReLU作为激活函数。

### 输出层和损失函数

## 神经网络的训练

## 卷积神经网络

## 循环神经网络

## 自编码器