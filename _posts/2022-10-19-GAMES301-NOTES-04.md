---
layout: article
title: GAMES301课程笔记04-无翻转参数化方法(初始无翻转)
tags: ["CG", "GAMES301", "Geometry Processing"]
key: GAMES301-04
aside:
  toc: true
sidebar:
  nav: GAMES301
---

> 这个系列是GAMES301-曲面参数化([GAMES 301: Surface Parameterization](http://staff.ustc.edu.cn/~renjiec/GAMES301/index.html))的同步课程笔记。课程内容涉及曲面参数化的基本理论与问题描述、面向离散网格的参数化、基于基函数表示的连续参数化、共形参数化、曲面参数化的应用等。本节课主要介绍初始无翻转情况下的的无翻转参数化方法。
<!--more-->

计算无翻转参数化的另一种思路是以一个初始无翻转的参数化作为起点，然后通过不断优化的方式来修正几何扭曲。

<div align=center>
<img src="https://i.imgur.com/j1zAZFa.png" width="80%">
</div>

这类方法一般会以[Tutte's embedding](/2022/10/14/GAMES301-NOTES-02.html#tuttes-embedding)作为初始化，然后在无翻转映射的空间中通过迭代的方式来求解几何扭曲的最小化问题。整个求解流程可以抽象为下图：

<div align=center>
<img src="https://i.imgur.com/YGkjoi2.png" width="80%">
</div>

为了保证优化过程中不会出现翻转的现象，在整个目标函数中还会加入一个**障碍函数(barrier function)**。当三角形趋向于退化或是翻转时，障碍函数会快速达到无穷大从而避免这种情况。除了单独设置障碍函数外还可以使用几何扭曲的度量函数来实现类似的功能，比如说MIPS函数就可以避免退化的情况：

$$
\frac{\sigma_1}{\sigma_2} + \frac{\sigma_2}{\sigma_1} = \frac{\sigma_1^2 + \sigma_2^2}{\sigma_1 \sigma_2} = \frac{\Vert \mathbf{J}_t \Vert_F^2}{\det \mathbf{J}_t}
$$

<div align=center>
<img src="https://i.imgur.com/s97vq1f.png" width="80%">
</div>

而在计算优化的方向时，大部分的算法都使用了局部二次近似这样的技巧。具体来说，目标函数$E(\mathbf{x})$可以使用泰勒展开来得到二阶近似：

$$
\tilde{E}(\mathbf{x}) = E(\mathbf{x}_i) + (\mathbf{x} - \mathbf{x}_i)^T \nabla E + \frac{1}{2} (\mathbf{x} - \mathbf{x}_i)^T H_i (\mathbf{x} - \mathbf{x}_i)
$$

这样近似后的目标函数就是一个凸函数，可以使用各种通用求解器来进行求解。需要注意的是二次项$H_i$不一定严格等于目标函数的Hessian矩阵，各种算法在实现时会使用不同的技巧来估计真实的Hessian矩阵。

<div align=center>
<img src="https://i.imgur.com/wYnh3OP.png" width="80%">
</div>

根据优化求解器的不同，参数化方法可以大致分为**一阶方法(first-order methods)**、**伪牛顿法(quasi-Newton methods)**以及**二阶方法(second-order methods)**。

<div align=center>
<img src="https://i.imgur.com/YquKwml.png" width="80%">
</div>

在求解优化问题时需要保证三角形不会出现翻转。假设三角形上三个顶点坐标分别为$\mathbf{u}_1$、$\mathbf{u}_2$、$\mathbf{u}_3$，每个顶点的位移分别为$\mathbf{v}_1 \alpha$、$\mathbf{v}_2 \alpha$、$\mathbf{v}_3 \alpha$，其中$\alpha$为优化的步长。当三角形的面积发生退化时有：

$$
\det
\begin{bmatrix}
(\mathbf{u}_2 + \mathbf{v}_2 \alpha) - (\mathbf{u}_1 + \mathbf{v}_1\alpha) \\
(\mathbf{u}_3 + \mathbf{v}_3 \alpha) - (\mathbf{u}_1 + \mathbf{v}_1\alpha) \\
\end{bmatrix}
= 0
$$

对行列式进行展开可以得到关于$\alpha$的二次多项式，其最小正数根即为迭代时的最大步长。因此在整个网格上最大优化步长即为所有三角形上二次多项式的最小正根。当然在进行优化时还要保证步长满足[Wolfe condition](https://en.wikipedia.org/wiki/Wolfe_conditions)。

<div align=center>
<img src="https://i.imgur.com/j4eKZUq.png" width="80%">
</div>

而常用的优化终止条件与一般的优化问题基本一致。

<div align=center>
<img src="https://i.imgur.com/hL7UcWC.png" width="80%">
</div>

## First-Order Methods

一阶方法只利用了目标函数的一阶导数，相当于把二阶项$H_i$直接取0。

<div align=center>
<img src="https://i.imgur.com/YquKwml.png" width="80%">
</div>

## Quasi-Newton Methods

## Second-Order Methods

## Reference

- [Lecture 04：无翻转参数化方法 – 初始无翻转](https://www.bilibili.com/video/BV18T411P7hT/?p=4&vd_source=7a2542c6c909b3ee1fab551277360826)