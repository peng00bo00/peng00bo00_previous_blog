---
layout: article
title: 深蓝学院-GNN课程笔记03-图嵌入
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-03
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图嵌入的相关内容。
<!--more-->

## 图嵌入的通用框架

**图嵌入(graph embedding)**的目的是将图上的每个节点映射为一个向量，这个向量也称为**节点嵌入(node embedding)**。最简单的嵌入方法是使用邻接矩阵的对应列作为嵌入，但由于邻接矩阵往往是非常稀疏的这样的嵌入方式在计算和存储上效率都很低。因此我们希望能够在图上找到一个低维的节点嵌入，使得嵌入后的节点向量能够保留原始图上的一些信息。

<div align=center>
<img src="https://i.imgur.com/5Bv2IyJ.png" width="70%">
</div>

第一代图嵌入方法是基于图拉普拉斯矩阵的谱分解来实现嵌入。它的核心思想是对拉普拉斯矩阵进行矩阵分解，从而实现对数据进行降维。这种嵌入方法的主要缺陷在于它具有非常高的计算复杂度，因此难以应用到大规模的图上。

<div align=center>
<img src="https://i.imgur.com/ZfgmYzk.png" width="70%">
</div>

第二代图嵌入方法则是来源于自然语言处理中的**词向量(word2vec)**技术。词向量是利用语料中词的共现信息来对词进行嵌入，它的基本思想是同时出现的词会具有相似的语义。把词向量的思想应用到节点嵌入中就得到了第二代图嵌入技术，和第一代嵌入技术相比第二代图嵌入技术通常比较高效而且具有更好的拓展性。

<div align=center>
<img src="https://i.imgur.com/wt4Rosr.png" width="60%">
</div>

从宏观上来看，图嵌入需要解决两个基本问题：需要保留图上的哪些信息以及如何取保留这些信息。常用的信息包括共现信息、结构角色、社区信息以及节点状态等，而在对信息进行重构时则需要保证嵌入后的结果与嵌入前尽可能一致。因此图嵌入的通用框架需要4个关键组件：

- 映射函数$f$将图域节点映射到嵌入域$\mathbb{R}^d$；
- 提取图上关键信息$I$的信息提取器；
- 利用嵌入域重新构造所提取到的信息，构造的信息记为$I'$；
- 对提取到的信息$I$和重构信息$I'$进行优化，学习映射函数$f$的参数。

<div align=center>
<img src="https://i.imgur.com/I17drUV.png" width="60%">
</div>

## 简单图嵌入

### DeepWalk

DeepWalk是基于节点共现信息来实现图嵌入的方法。对于没有方向且只包含节点连接信息的简单图，我们可以利用图上的**随机游走(random walk)**来模拟共现语料的效果。定义当前节点$v^{(t)}$转移到与它相邻的其它节点的概率为：

$$
P(v^{(t+1)} \vert v^{(t)}) = 
\left\{
\begin{aligned}
& \frac{1}{d(v^{(t)})} & & v^{(t+1)} \in N(v^{(t)}) \\
& 0 & & \text{otherwise}
\end{aligned}
\right.
$$

也就是说在$v^{(t)}$处会以均匀概率转移到和它相邻的其它节点上。以这样的方式在图上进行采样就得到了一系列节点构成的序列作为训练数据。对于训练数据中的中心节点和与它相邻的上下文节点，我们构造2个映射函数：

$$
f_{cen} (v_i) = W_{cen} e_i
$$

$$
f_{con} (v_i) = W_{con} e_i
$$

因此对于共现信息$I_W = \{ (v_{cen}, v_{con}) \}$，中心节点和上下文节点的共现概率可以表示为：

$$
P(v_{con} \vert v_{cen}) = \frac{\exp \{ f_{con} (v_{con})^T f_{cen} (v_{cen}) \}}{\sum_v \exp \{ f_{con} (v)^T f_{cen} (v_{cen}) \}}
$$

将序列中的共现概率累乘起来可以得到似然函数：

$$
L(W_{cen}, W_{con}) = \prod_{I_W} \frac{\exp \{ f_{con} (v_{con})^T f_{cen} (v_{cen}) \}}{\sum_v \exp \{ f_{con} (v)^T f_{cen} (v_{cen}) \}}
$$

我们可以通过最大化似然函数来重构嵌入后的共现信息。在实际应用中更常见的形式是最小化对数损失来实现训练过程：

$$
\min -\sum_{I_W} \log \frac{\exp \{ f_{con} (v_{con})^T f_{cen} (v_{cen}) \}}{\sum_v \exp \{ f_{con} (v)^T f_{cen} (v_{cen}) \}}
$$

DeepWalk训练时的主要难点在于分母项$\sum_v \exp \{ f_{con} (v)^T f_{cen} (v_{cen}) \}$，当图上的节点很多时分母项需要计算中心节点和每个节点嵌入向量的内积并求和。为了解决这个问题实践中往往通过分层softmax或者负样本采样的方式来对计算过程进行加速。

## 复杂图嵌入