---
layout: article
title: 深蓝学院-GNN课程笔记04-图神经网络
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-04
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图神经网络的相关内容。
<!--more-->

## 图神经网络

在前面的课程中我们介绍过图嵌入的发展过程，包括以数据降维为代表的第一代图嵌入技术以及以word2vec为基础的第二代图嵌入技术，而在本节课中我们将介绍以**图神经网络(graph neural network, GNN)**为代表的第三代图嵌入技术。

<div align=center>
<img src="https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png" width="70%">
</div>

GNN旨在将深度神经网络的相关方法应用到图上。由于图不是规则的数据结构，在GNN中需要一些额外的操作包括侧重于节点的图滤波以及侧重于图整体的图池化。

<div align=center>
<img src="https://i.imgur.com/97TKBhS.png" width="70%">
</div>

## 谱图论

图滤波的基础是[谱图论](/2021/09/04/GNN-NOTES-01.html#谱图论与图信号处理)。在前面的课程中我们介绍过拉普拉斯矩阵描述了图上每个节点与它相邻节点之间的差异：

$$
h = L f = Df - Af
$$

$$
h_i = \sum_{v_j \in N(v_i)} (f_i - f_j)
$$

$$
f^T L f = \frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in N(v_i)} (f_i - f_j)^2
$$

同时拉普拉斯矩阵还描述了图的频域性质，图上的傅里叶变换由拉普拉斯矩阵给出：

$$
F = U^T f
$$

$$
f = U F
$$

$$
L = U \Lambda U^T
$$

其中$f$和$F$分别是图信号的空域和频域表示；$U$为拉普拉斯矩阵$L$的特征向量矩阵；$\Lambda$为$L$的特征值。由于拉普拉斯矩阵$L$是(半)正定对称的，它的特征值为非负数也称为图的频率。

## 图滤波

### 图谱滤波

最基本的图滤波方式是基于谱图论的图谱滤波。对于图信号$f$我们首先利用图傅里叶变换得到频率：

$$
\hat{f} = U^T f
$$

$\hat{f}$中的每个值表示该信号在不同频率上的相应。我们可以利用一个滤波器来对频率信号进行处理：

$$
\hat{g}(\Lambda) = 
\begin{bmatrix}
\hat{g}(\lambda_0) & & & \\
& \hat{g}(\lambda_1) & & \\
& & \ddots & \\
& & & \hat{g}(\lambda_{N-1}) \\
\end{bmatrix}
$$

最后利用图逆傅里叶变换来重构图信号：

$$
f' = U \hat{g}(\Lambda) U^T f
$$

<div align=center>
<img src="https://i.imgur.com/S3PoIKH.png" width="90%">
</div>

显然图谱滤波的核心是设计滤波器$\hat{g}(\Lambda)$，这可以通过训练一个前馈神经网络来从数据中进行学习。然而这种方式的主要缺陷在于它具有过大的计算复杂度：对于包含$N$个节点$d_1$维信号的图，如果要输出$d_2$维数量的信号，模型的参数量为$N \times d_1 \times d_2$；同时图谱滤波还需要对图进行特征值分解，这又提高了计算的复杂度。因此图谱滤波很难直接应用到大规模图上。

### Chebyshev多项式

为了降低计算复杂度，我们可以利用一个多项式来拟合$\hat{g}(\Lambda)$：

$$
\hat{g}(\Lambda) = 
\begin{bmatrix}
\sum_k \theta_k \lambda_0^k & & & \\
& \sum_k \theta_k \lambda_1^k & & \\
& & \ddots & \\
& & & \sum_k \theta_k \lambda_{N-1}^k \\
\end{bmatrix}
$$

其中参数$\theta_k$由不同频率所共享。通过这样定义的滤波器我们还可以避免对图进行特征值分解：

$$
U \hat{g}(\Lambda) U^T f = \sum_k \theta_k U \Lambda^k U^T f = \sum_k \theta_k L^k f
$$

在工程中更常用的方式是使用[Chebyshev多项式](https://en.wikipedia.org/wiki/Chebyshev_polynomials)来代替标准的多项式，这样使得训练过程更加稳定。Chebyshev多项式可以通过递归来定义：

$$
T_0 (x) = 1, T_1(x) = x
$$

$$
T_k (x) = 2x T_{k-1}(x) - T_{k-2}(x)
$$

由于Chebyshev多项式是定义在区间[-1, 1]上的，在使用它进行滤波时首先需要对频率进行缩放：

$$
\tilde \lambda_i = \frac{2 \lambda_i}{\lambda_{\max}} - 1
$$

这样使用Chebyshev多项式进行图谱滤波可以表示为：

$$
U \hat{g}(\Lambda) U^T f = \sum_k \theta_k T_k(\tilde L) f
$$

$$
\tilde L = \frac{2 L}{\lambda_{\max}} - I
$$

与传统图谱滤波相比使用Chebyshev多项式作为滤波器具有更少的参数、训练时无需矩阵特征值分解、同时训练过程也更加稳定。

### GCN

在GCN中我们进一步只考虑一阶的Chebyshev多项式



## 图池化