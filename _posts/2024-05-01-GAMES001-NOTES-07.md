---
layout: article
title: GAMES001课程笔记07-概率论I
tags: ["CG", "GAMES001", "Math"]
key: GAMES001-07
aside:
  toc: true
sidebar:
  nav: GAMES001
---

> 这个系列是GAMES001-图形学中的数学([GAMES 001: Mathematics in Computer Graphics](https://games-cn.org/games001/))的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或"手册"，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍概率论的基础知识。
<!--more-->

## 概率

### 从频率到概率

"频率"和"概率"是两个密切相关的概念。我们把$$n$$次试验中事件$$A$$发生的次数除以试验的次数称为"频率"，它是一个落在区间[0， 1]上的实数，记作$$f_n (A)$$。而概率则是定义在样本空间$$S$$上的函数，我们为每一个事件$$A$$赋予一个实数$$P(A)$$，称$$P(A)$$为事件$$A$$的概率。概率$$P(A)$$需要满足非负性、规范性以及可列可加性，同时我们令概率$$P(A)$$等于试验次数$$n$$趋于无穷时频率的极限：

$$
P(A) = \lim_{n \rightarrow \infty} f_n (A)
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Fre7qk6.png" width="100%">
</div>

### 古典概型

在古典概型中，我们认为样本空间只包含有限多的元素而且每个基本事件发生的可能性相同，因此古典概型也称为等可能概型。根据古典概型的定义，事件$$A$$的概率等于它包含的基本事件的个数除以基本事件的总数。我们在中学数学中学习的排列组合问题就与古典概型密切相关。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/CJVZMz5.png" width="100%">
</div>

### 条件概率

在概率的基础上我们可以定义条件概率。我们定义事件$$A$$已经发生的基础上，事件$$B$$发生的条件概率为

$$
P(B \vert A) = \frac{P(AB)}{P(A)}
$$

其中$$P(AB)$$为事件$$A$$和事件$$B$$同时发生的概率。

基于条件概率，我们可以进一步推导出全概率公式：

$$
P(A) = P(A \vert B_1) P(B_1) + P(A \vert B_2) P(B_2) + \cdots + P(A \vert B_n) P(B_n)
$$

其中，事件$$B_1, \dots B_n$$是对样本空间的一个划分。

另一个重要的概率公式是贝叶斯公式：

$$
P(B_i \vert A) = \frac{P(A \vert B_i) P(B_i)}{\sum_{j=1}^n P(A \vert B_j) P(B_j)}
$$

贝叶斯公式在数据分析、机器学习等领域都有着重要的应用。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/v2SJuep.png" width="100%">
</div>

## 随机变量

古典概型只适用于基本事件具有相同发生可能性的情况，对于更复杂的问题我们需要引入随机变量的概念。我们定义随机变量$$X$$是定义在样本空间$$S$$上的实值单值函数。根据离散和连续的情况，随机变量可以分为离散型随机变量和连续型随机变量。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/xfX2ug4.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/hsrFlIu.png" width="100%">
</div>

### 随机变量的数字特征

#### 数学期望

随机变量的数学期望定义为随机变量与其对应的概率相乘再求和：

$$
\begin{aligned}
E(X) &= \sum_k^\infty x_k p_k = \sum_k^\infty P(X \geq k) \\
&= \int_{-\infty}^\infty x f(x) \ \mathrm{d} x
\end{aligned}
$$

数学期望的常用性质可以参考下面的ppt。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/YRD8Fxt.png" width="100%">
</div>

#### 方差

方差描述了随机变量偏离其均值的程度，它的计算公式为

$$
D(X) = E \{ [X - E(X)]^2 \} = E(X^2) - [E(X)]^2
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/hvAVTfE.png" width="100%">
</div>

#### 协方差

协方差描述了两个随机变量$$X$$和$$Y$$之间的相关性，其计算公式为

$$
\begin{aligned}
\text{cov} (X, Y) &= E \{ [X - E(X)] [Y - E(Y)] \} \\
&= E (XY) - E(X) E(Y)
\end{aligned}
$$

在协方差的基础上，我们还可以定义$$X$$和$$Y$$的相关系数

$$
\rho_{XY} = \frac{\text{cov} (X, Y)}{\sigma (X) \sigma (Y)}
$$

协方差的一个性质是相互独立的两个随机变量它们的协方差为0。但需要注意这个命题的逆命题不成立，即协方差为0不能推导出两个随机变量相互独立。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/iic0sJa.png" width="100%">
</div>

协方差的概念还可以推广到多维的情况，此时我们可以通过协方差矩阵来描述每一对随机变量之间的相关性。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/l6JREYg.png" width="100%">
</div>

#### 矩

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/6aOUKYd.png" width="100%">
</div>

### 泊松分布

泊松分布是一种重要的离散型随机变量分布。令$$X$$表示一段区间内某个事件发生的次数，同时该事件满足如下假定：

1. 一个事件的发生不影响其它事件的发生，即事件相互独立。
2. 每个事件发生的概率是相同的。
3. 两个事件不能在同一时刻发生。
4. 一个区间内事件的概率与区间大小成比例。

设单位区间内事件发生的平均次数为$$\lambda$$，则发生$$k$$次事件的概率为

$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/FtVsiYk.png" width="100%">
</div>

### 指数分布

指数分布是一种重要的连续型随机变量分布。参数为$$\lambda$$的指数分布的概率密度函数为

$$
f(x) = \lambda e^{-\lambda x}, \ \ \ x \geq 0
$$

指数分别的一个重要性质是无记忆性，即

$$
P \{ X \gt s + t \vert X \gt s \} = P \{ X \gt t \}
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/IrytMYF.png" width="100%">
</div>

## 大数定律和中心极限定理

### 大数定律

[弱大数定律](https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law)指出，相互独立且服从相同分布的随机变量序列$$\{ X_i, X_2, ... \}$$，它们的均值会**依概率**收敛到所属分布的均值上：

$$
\bar{X} = \frac{1}{n} \sum_{k=1}^n X_k \xrightarrow{P} \mu
$$

而伯努利大数定律则指出，事件发生的频率依概率收敛于事件的总体概率

$$
\lim_{n \to\infty} P \bigg\{ \bigg\vert \frac{f_A}{n} - p \bigg\vert \lt \varepsilon \bigg\} = 1
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/4NrNTcp.png" width="100%">
</div>

### 中心极限定理

[中心极限定理](https://en.wikipedia.org/wiki/Central_limit_theorem#Classical_CLT)指出，独立同分布的随机变量$$\{ X_i, X_2, ... \}$$的和会趋向于正态分布$$N(n \mu, n \sigma^2)$$：

$$
\lim_{n \to \infty} \sum_{k=1}^n X_k \rightarrow N(n\mu, n \sigma^2)
$$

更常见的形式为：

$$
\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n X_k \rightarrow N \bigg( \mu, \frac{\sigma^2}{n} \bigg)
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/XhpZ44D.png" width="100%">
</div>

对于非独立同分布的情况，[Lyapunov定理](https://en.wikipedia.org/wiki/Central_limit_theorem#Lyapunov_CLT)指出只要随机变量是相互独立的，当满足一定条件时，它们的和仍然能够收敛到正态分布。

此外，[De Moivre–Laplace定理](https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem)指出二项分布的极限是标准正态分布。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/SklIpYW.png" width="100%">
</div>

## Reference

- [Lecture 08: 概率论(一)](https://www.bilibili.com/video/BV1MF4m1V7e3?p=8&vd_source=7a2542c6c909b3ee1fab551277360826)