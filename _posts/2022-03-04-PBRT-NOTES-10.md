---
layout: article
title: PBRT读书笔记10-Monte Carlo Integration
tags: ["PBRT", "Rendering", "CG"]
key: PBRT-10
aside:
  toc: true
sidebar:
  nav: PBRT
---

> 这个系列是[Physically Based Rendering: From Theory To Implementation](https://pbr-book.org/)的读书笔记，本节主要介绍Monte Carlo积分的相关知识。
<!--more-->

在渲染过程中我们经常需要去计算各种各样的高维数值积分。这些被积函数往往十分复杂，甚至没有解析形式。计算这类函数的积分一般需要在被积范围内进行采样，因此这种方法称为**Monte Carlo积分(Monte Carlo integration)**。

## Background and Probability Review

在正式介绍Monte Carlo积分前首先简要复习一下概率论和随机变量的相关知识。对于随机变量我们使用斜体$X$来表示，把某个函数$f$作用在$X$上时我们会得到一个新的随机变量$Y = f(X)$。离散型随机变量的**累计概率分布函数(cumulative distribution function, CDF)**定义为：

$$
P(x) = Pr(X \leq x)
$$

### Continuous Random Variables

在渲染中更为常用的是连续型随机变量，其中[0, 1)区间上均匀分布的随机变量记为$\xi$。$\xi$是一个非常重要的随机变量，利用$\xi$可以生成更加复杂分布的随机变量。我们定义连续型随机变量的**概率密度函数(probability density function, PDF)**为CDF的微分：

$$
p(x) = \frac{d \ P(x)}{d \ x}
$$

对于均匀分布的随机变量$\xi$，它的PDF为

$$
p(x) = 
\left\{
\begin{aligned}
& 1 & x \in [0, 1) \\
& 0 & \text{otherwise}
\end{aligned}
\right.
$$

需要注意的是任意PDF必须满足非负而且积分为1。在给定区间[a, b]上PDF的积分即为随机变量落在该范围上的概率：

$$
P(x \in [a, b]) = \int_a^b p(x) \ dx
$$

### Expected Values and Variance

我们定义函数$f$在给定概率分布$p(x)$上的**期望(expected value)**为$f(x)$与$p(x)$的积分：

$$
E_p [f(x)] = \int_D f(x) p(x) \ dx
$$

定义$f$的**方差(variance)**为$f(x)$偏离自身期望的程度：

$$
V[f(x)] = E \bigg[ \big( f(x) - E[f(x)] \big)^2 \bigg] = E[(f(x))^2] - E[f(x)]^2
$$

期望和方差的常用性质如下：

$$
E[a f(x)] = a E[f(x)]
$$

$$
E\bigg[ \sum_i f(x_i) \bigg] = \sum_i E[f(x_i)]
$$

$$
V[a f(x)] = a^2 V[f(x)]
$$

对于相互独立的随机变量，它们的方差具有可加性：

$$
\sum_i V[f(x_i)] = V \bigg[ \sum_i f(x_i) \bigg]
$$

## The Monte Carlo Estimator

对于一元函数$f(x)$，它在[a, b]上的积分$\int_a^b f(x) dx$可以通过[a, b]上均匀分布随机变量$X_i$的函数来估计：

$$
F_N = \frac{b-a}{N} \sum_{i=1}^N f(X_i)
$$

实际上$F_N$的期望恰好等于积分：

$$
\begin{aligned}
E[F_N] &= E \bigg[ \frac{b-a}{N} \sum_{i=1}^N f(X_i) \bigg] \\
&= \frac{b-a}{N} \sum_{i=1}^N E[f(X_i)] \\
&= \frac{b-a}{N} \sum_{i=1}^N \int_a^b f(x) p(x) \ dx \\
&= \frac{1}{N} \sum_{i=1}^N \int_a^b f(x) \ dx \\
&= \int_a^b f(x) \ dx
\end{aligned}
$$

实际上我们并不需要将随机变量限制为均匀分布，对于任意分布$p(x)$只需要对估计进行一定的变形：

$$
F_N = \frac{1}{N} \sum_{i=1}^N \frac{f(X_i)}{p(X_i)}
$$

同样可以证明上式的期望等于积分值：

$$
\begin{aligned}
E[F_N] &= E \bigg[ \frac{1}{N} \sum_{i=1}^N \frac{f(X_i)}{p(X_i)} \bigg] \\
&= \frac{1}{N} \sum_{i=1}^N \int_a^b \frac{f(x)}{p(x)} p(x) \ dx \\
&= \frac{1}{N} \sum_{i=1}^N \int_a^b f(x) \ dx \\
&= \int_a^b f(x) \ dx
\end{aligned}
$$

Monte Carlo积分同样可以拓展到多元函数上。假设我们想要计算一个三重积分：

$$
\int_{z_0}^{z_1} \int_{y_0}^{y_1} \int_{x_0}^{x_1} f(x, y, z) \ dx \ dy \ dz
$$

为了计算积分值需要从$[x_0, x_1] \times [y_0, y_1] \times [z_0, z_1]$进行均匀采样，每个样本的PDF均为：

$$
\frac{1}{x_1 - x_0} \cdot \frac{1}{y_1 - y_0} \cdot \frac{1}{z_1 - z_0}
$$

因此使用Monte Carlo积分得到的估计值为：

$$
\frac{(x_1 - x_0)(y_1 - y_0)(z_1 - z_0)}{N} \sum_i f(X_i)
$$

对于Monte Carlo积分而言样本数$N$可以是任意的。这样的好处在于无论被积函数的维度如何我们都可以很好地控制计算复杂度，这也是Monte Carlo积分优于很多其它数值积分方法的原因之一。

除此之外还需要考虑的是Monte Carlo积分的精度，实际上可以证明Monte Carlo积分误差的收敛速度为$O(\sqrt{N})$且与维数无关。对于一维的情况很多数值积分方法都有着更好的收敛速度，但随着维数的增加这些数值积分的精度会显著下降。因此在多维的情况下Monte Carlo积分几乎是唯一可行的积分方法。

## Sampling Random Variables

Monte Carlo积分的难点在于如何对各种不同类型的分布进行采样，在本节中我们暂时只介绍最简单的两种采样方法，更多复杂的采样方法会留到后面的章节再进行介绍。

### The Inversion Method

最基本的采样方法是**逆变换采样(inverse transform sampling)**。对于任意分布的随机变量$X$可以证明把CDF作用到$X$上后会得到均匀分布，即$P(X) \sim U(0, 1)$。因此我们只需要从均匀分布进行采样，然后利用$P(x)$的反函数即可获得来自$X$分布的样本。

<div align=center>
<img src="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/discrete-cdf.svg" width="40%">
<img src="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/discrete-inversion.svg" width="40%">
</div>

逆变换采样的步骤如下：

- 计算随机变量$X$的CDF：$$P(x) = \int_0^x p(x') \ dx'$$
- 计算CDF的反函数$P^{-1}(x)$
- 从均匀分布$U(0, 1)$进行采样，得到样本$\xi$
- 利用CDF的反函数得到来自$X$分布的样本$X_i = P^{-1}(\xi)$

#### Power Distribution

对于**幂分布(power distribution)**，它的PDF可以表示为：

$$
p(x) = c x^n
$$

利用PDF的归一化条件可以求得常数$c$为：

$$
\int_0^1 c x^n \ dx = 1 \ \Leftrightarrow \ c = n+1
$$

因此幂分布的CDF为：

$$
P(x) = \int_0^x p(x') \ dx' = x^{n+1}
$$

对应的反函数为：

$$
P^{-1} (x) = \sqrt[n+1]{x}
$$

因此要获得幂分布的样本只需要对均匀分布的样本$\xi$进行逆变换即可：

$$
X = \sqrt[n+1]{\xi}
$$

#### Exponential Distribution

对于参数为$a$的指数分布，其CDF为：

$$
P(x) = \int_0^x a e^{-ax'} \ dx' = 1 - e^{-a x}
$$

对应的反函数为：

$$
P^{-1} (x) = -\frac{\ln(1-x)}{a}
$$

因此获取指数分布的样本只需要利用变换：

$$
X = -\frac{\ln(1-\xi)}{a}
$$

#### Piecewise-Constant 1D Functions

很多时候我们需要考虑分段的概率分布。假设每一段的大小是均匀的$\Delta = \frac{1}{N}$，此时未归一化的PDF可以表示为：

$$
f(x) = 
\left\{
\begin{aligned}
& v_0 & x_0 \leq x \lt x_1 \\
& v_1 & x_1 \leq x \lt x_2 \\
& \vdots
\end{aligned}
\right.
$$

利用PDF的归一化条件可以得到归一化常数$c$：

$$
c = \int_0^1 f(x) \ dx = \sum_{i=0}^{N-1} v_i \Delta = \sum_{i=0}^{N-1} \frac{v_i}{N}
$$

因此CDF在不同间隔上的值为：

$$
P(x_0) = 0
$$

$$
P(x_1) = P(x_0) + \frac{v_0}{c N}
$$

$$
P(x_2) = P(x_1) + \frac{v_1}{c N}
$$

$$
P(x_i) = P(x_{i-1}) + \frac{v_{i-1}}{c N}
$$

<div align=center>
<img src="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/constant-func-pdf.svg" width="40%">
<img src="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/constant-func-cdf.svg" width="40%">
</div>

进行逆变换采样时需要计算$\xi$位于的区间使得$P(x_i) \leq \xi$且$\xi \leq P(x_{i+1})$。在已知CDF的情况下可以使用二分查找的方法来加速计算过程。

### The Rejection Method

对于某些分布的CDF直接进行计算可能非常复杂甚至没有解析形式，此时就不能使用逆变换采样。在这种情况下可以使用**接受-拒绝采样(acceptance-rejection sampling)**来生成样本。接受-拒绝采样不需要计算概率分布的CDF，只需要目标分布$f(x)$和另一个已知分布$p(x)$的PDF即可，同时两个分布的PDF需要满足$f(x) \leq c p(x)$，其中$c$为常数。

接受-拒绝采样的步骤如下：

- 从已知分布$p(x)$上生成样本$X$
- 从均匀分布$U(0, 1)$上采样一个随机数$\xi$
- 如果$\xi \leq \frac{f(X)}{c p(X)}$则接受$X$作为样本，否则回到第一步重新进行采样

从几何意义上讲接受-拒绝采样会生成随机变量对$(X, \xi)$，如果点$(X, \xi \cdot c p(X))$位于曲线$f(X)$的下方则接受样本。接受-拒绝采样的一个优势在于它与随机变量的维数无关，只要能够计算PDF即可。

<div align=center>
<img src="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/rejection-sample-func.svg" width="50%">
</div>

#### Rejection Sampling a Unit Circle

接受-拒绝采样的经典应用是生成圆内均匀分布的样本。我们只需要在正方形区域内进行均匀采样，然后拒绝掉落在圆外的样本即可。

<div align=center>
<img src="https://pbr-book.org/3ed-2018/Monte_Carlo_Integration/rejection-sample-circle.svg" width="30%">
</div>

在PBRT中使用`RejectionSampleDisk()`函数来描述单位圆内进行均匀采样的过程：

```cpp
Point2f RejectionSampleDisk(RNG &rng) {
    Point2f p;
    do {
        p.x = 1 - 2 * rng.UniformFloat();
        p.y = 1 - 2 * rng.UniformFloat();
    } while (p.x * p.x + p.y * p.y > 1);
    return p;
}
```

## Metropolis Sampling

**Metropolis采样(Metropolis sampling)**是一种针对非负函数$f(x)$进行采样的方法，其中可以认为$f(x)$是归一化前的概率密度。和前面介绍过的采样方法相比，Metropolis采样不需要积分也不需要计算函数的反函数，只要能对$f(x)$求值即可。而且Metropolis采样不会拒绝生成的样本，在每一轮的采样过程中都可以得到新的数据。因此Metropolis采样比前面介绍过的采样方法都要高效。

Metropolis采样的缺陷在于它是一个迭代的算法，换句话说连续的样本之间是存在一定关联的。它的另一个缺陷在于当采样数比较少时生成的样本可能无法覆盖整个定义域，因此很多降低方差的技术不能直接用在Metropolis采样中。

### Basic Algorithm

Metropolis采样的目标是从给定的函数$f: \Omega \rightarrow \mathbb{R}$上采样出一系列样本$X_i$。设初始的样本为$X_0$，在每一轮采样过程中样本$X_i$都是由前一个样本$X_{i-1}$经过一个随机**突变(mutation)**来获得，突变的结果记为$X'$。我们可以接受或拒绝突变的样本，因此$X_i$的取值为$X_{i-1}$或$X'$。在这种采样模式下$X_i$会达到一种平衡态分布，这个概率分布称为**平稳分布(stationary distribution)**。在极限情况下$X_i$的概率分布即为$f(x)$定义的概率分布：$$p(x) = \frac{f(x)}{\int_\Omega f(x) \ d\Omega}$$。

显然Metropolis采样的核心是设计突变函数以及接受突变的策略。突变函数可以有不同的形式，比如说对当前样本$X$进行扰动或是直接生成一个新的样本都可以。假设我们已经有了某种突变策略，接下来需要考虑从$X$突变为$X'$的概率$T(X \rightarrow X') = P(X' \vert X)$，称为**状态转移函数(transition function)**。在此基础上我们可以定义一个接受概率$a(X \rightarrow X')$表示我们是否要接受突变的样本，从而保证样本$X$的分布正比于$f(x)$。当系统达到平衡态时任意两个状态相互转换的概率必须相等，即：

$$
f(X) \ T(X \rightarrow X') \ a(X \rightarrow X') = f(X') \ T(X' \rightarrow X) \ a(X' \rightarrow X)
$$

上式称为**细致平衡条件(detailed balance)**。当$f$和$T$确定时我们可以得到接受概率$a$的表达式：

$$
a(X \rightarrow X') = \max \bigg( 1, \frac{f(X') \ T(X' \rightarrow X)}{f(X) \ T(X \rightarrow X')} \bigg)
$$

容易验证上式定义的接受概率$a$满足细致平衡条件。如果进一步假设$X$和$X'$相互突变的概率是相等的，则可以得到更简单的接受概率：

$$
a(X \rightarrow X') = \max \bigg( 1, \frac{f(X')}{f(X)} \bigg)
$$

这样我们就得到了最基本的Metropolis采样算法，它的伪代码形式如下：

```
X = X0
for i = 1 to n
    X' = mutate(X)
    a = accept(X, X')
    if (random() < a)
        X = X'
    record(X)
```

当$f(X')$的值比较小时Metropolis采样会有很大的概率拒绝突变，这会使得只有很少的样本会分布在函数值比较小的区域。为了克服这个问题我们可以使用求期望的方法来改进标准的Metropolis采样算法。此时在每一轮采样中需要同时记录$X$和$X'$，并且为它们赋予相应的概率(权重)。改进后的Metropolis采样算法伪代码如下：

```
X = X0
for i = 1 to n
    X' = mutate(X)
    a = accept(X, X')
    record(X, 1 - a)
    record(X', a)
    if (random() < a)
        X = X'
```

### Choosing Mutation Strategies

### Start-up Bias

### 1D Setting

### Estimating Integrals with Metropolis Sampling

## Transforming between Distributions

## 2D Sampling with Multidimensional Transformations

## Russian Roulette and Splitting

## Careful Sample Placement

## Bias

## Importance Sampling

## Reference

- [13 Monte Carlo Integration](https://pbr-book.org/3ed-2018/Monte_Carlo_Integration)