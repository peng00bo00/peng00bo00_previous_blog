---
layout: article
title: 深蓝学院-GNN课程笔记06-图神经网络的可扩展性
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-06
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/course/376?source=1)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要介绍图神经网络的可扩展性。
<!--more-->

## 可扩展性

现实生活中的图神经网络往往会面临可拓展性上的问题。目前大规模的社交网络已经达到了上亿的用户级别，显然它们所对应的图规模是非常巨大的，而直接在这种超大规模的图上使用GNN是非常困难的。

<div align=center>
<img src="https://i.imgur.com/YGVEum6.png" width="50%">
</div>

以GCN和节点分类任务为例，我们可以把单层的图神经网络表示为：

$$
f_\text{GCN} (A, F; \Theta) = \hat{A} F \Theta
$$

假设图神经网络一共有$L$层，则训练损失函数可以表示为：

$$
L_\text{train} = \sum_{v_i \in V} l(f_\text{GCN} (A, F; \Theta)_i, y_i)
$$

$$
F^{(l)} = \hat{A} F^{(l-1)} \Theta^{(l-1)} 
$$

如果我们直接在全图上进行训练，存储所有层的特征所需要的空间复杂度为$O(L \cdot \vert V \vert \cdot d)$，其中$d$为每个节点的特征维数。显然对于超大规模的图光是存储特征的空间复杂度已经是难以接受的，更不用说进行神经网络前向和反向传播所需要的计算复杂度了。

因此要解决超大规模图上的训练问题一种直观的想法是采用批量训练的方式，每次只在图上选择一部分节点来进行训练。但这种方式也存在邻域爆炸的问题：假设图上每个节点的度平均值为$\text{deg}$，第$L$层的节点需要与它相邻的$\text{deg}$个节点的信息；再往下递推，第$L-1$层需要$\text{deg}^2$个节点的信息...每一层的前向计算过程可以表示为：

$$
F_i^{(l)} = \sum_{v_j \in \hat{N}(v_i)} \hat{A}_{i,j} F_j^{(l-1)} \Theta^{(l-1)} 
$$

<div align=center>
<img src="https://i.imgur.com/T3cXnq7.png" width="70%">
</div>

因此使用批量训练的方式所需要的空间复杂度为$O(\text{deg}^L \cdot d)$，对于超大规模的图而言仍然是不可接受的。

## 逐点采样法

## 逐层采样法

## 子图采样法