---
layout: article
title: GAMES001课程笔记14-深度学习
tags: ["CG", "GAMES001", "Math"]
key: GAMES001-14
aside:
  toc: true
sidebar:
  nav: GAMES001
---

> 这个系列是GAMES001-图形学中的数学([GAMES 001: Mathematics in Computer Graphics](https://games-cn.org/games001/))的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或"手册"，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍求解深度学习中生成式模型相关的数学知识。
<!--more-->

## 深度学习

深度学习作为当前最热门的话题之一，已经渗透到计算机科学的各个研究领域中。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/JWUry2U.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/MWJEAik.png" width="100%">
</div>

从数学知识的角度来看，深度学习的基础无非是概率论、线性代数以及数学优化。当然，从学习这些深度学习相关的数学基础到掌握当前最前沿的深度学习模型之间，仍然有非常大的距离。本节课的目标是从当前热门的生成式模型入手，介绍相关的数学知识。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/dt2eppu.png" width="100%">
</div>

### 损失函数

从[拟合](/2024/04/10/GAMES001-NOTES-05.html)的角度来看，深度学习的目标是从一系列已知的数据点中学习到一个能够很好拟合这些数据的模型。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/XBkb2XV.png" width="100%">
</div>

我们可以暂时忽略神经网络架构的各种细节，把神经网络视为一个关于参数$$\theta$$的函数$$f_{\theta} (x)$$。这样，模型训练的过程实际上就是最小化关于$$\theta$$的损失函数的过程，通常通过随机梯度下降法来实现。根据不同类型的问题，可以选择相应的损失函数来进行处理。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Z7FX19O.png" width="100%">
</div>

### 生成式模型

目前，生成式模型是整个AI领域中最为热门的研究方向。从概率的角度来看，每个数据样本都来自于某个概率分布。只要我们能够通过神经网络近似这个分布，就可以通过采样的方式生成新样本。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/24kn3mc.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/xQ6diI9.png" width="100%">
</div>

然而，生成式模型需要回答以下两个问题：

1. 如何度量真实数据分布$$p(x)$$与网络$$\pi_\theta (x)$$之间的相似程度？
2. 如何使用神经网络来表示高维数据的分布？

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/KyOsMaO.png" width="100%">
</div>

对于第一个问题，通常可以使用[KL散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)来进行处理，它度量了$$p$$和$$q$$两个概率分布之间的差异。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/67Njibg.png" width="100%">
</div>

假设$$p$$是一个已知的概率分布，则最小化$$p$$和$$q$$之间KL散度等价于对$$q$$进行最大似然估计。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/484kltM.png" width="100%">
</div>

## VAE

### 隐变量

KL散度可以用来解决度量概率分布之间相似度的问题。在此基础上，我们可以利用[变分自编码器(VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder)来对概率分布进行建模。VAE是生成式模型中的经典方法，它的核心在于构造一个服从标准正态分布的隐变量$$z \sim N(0, I)$$，且其维度要远小于真实数据$$x$$的维度。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/GT0PBdd.png" width="100%">
</div>

每个数据样本都对应着一个隐变量分布，它们之间的关系可以通过联合概率进行描述。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Xq8gzgM.png" width="100%">
</div>

在VAE模型中，数据分布$$p(x)$$和隐变量分布$$p(z)$$通过**编码器(encoder)**和**解码器(decoder)**关联在一起：

- 编码器将数据$$x$$映射为隐变量$$z$$，相当于计算条件概率$$p(z | x)$$。
- 解码器将隐变量$$z$$重建为数据$$x$$，相当于计算条件概率$$p(x | z)$$。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/os5LrLe.png" width="100%">
</div>


## Diffusion Model

## Reference

- [Lecture 15: 生成模型](https://www.bilibili.com/video/BV1MF4m1V7e3?p=14&vd_source=7a2542c6c909b3ee1fab551277360826)