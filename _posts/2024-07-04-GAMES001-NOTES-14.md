---
layout: article
title: GAMES001课程笔记14-深度学习
tags: ["CG", "GAMES001", "Math"]
key: GAMES001-14
aside:
  toc: true
sidebar:
  nav: GAMES001
---

> 这个系列是GAMES001-图形学中的数学([GAMES 001: Mathematics in Computer Graphics](https://games-cn.org/games001/))的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或"手册"，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍求解深度学习中生成式模型相关的数学知识。
<!--more-->

## 深度学习

深度学习作为当前最热门的话题之一，已经渗透到计算机科学的各个研究领域中。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/JWUry2U.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/MWJEAik.png" width="100%">
</div>

从数学知识的角度来看，深度学习的基础无非是概率论、线性代数以及数学优化。当然，从学习这些深度学习相关的数学基础到掌握当前最前沿的深度学习模型之间，仍然有非常大的距离。本节课的目标是从当前热门的生成式模型入手，介绍相关的数学知识。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/dt2eppu.png" width="100%">
</div>

### 损失函数

从[拟合](/2024/04/10/GAMES001-NOTES-05.html)的角度来看，深度学习的目标是从一系列已知的数据点中学习到一个能够很好拟合这些数据的模型。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/XBkb2XV.png" width="100%">
</div>

我们可以暂时忽略神经网络架构的各种细节，把神经网络视为一个关于参数$$\theta$$的函数$$f_{\theta} (x)$$。这样，模型训练的过程实际上就是最小化关于$$\theta$$的损失函数的过程，通常通过随机梯度下降法来实现。根据不同类型的问题，可以选择相应的损失函数来进行处理。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Z7FX19O.png" width="100%">
</div>

### 生成式模型

目前，生成式模型是整个AI领域中最为热门的研究方向。从概率的角度来看，每个数据样本都来自于某个概率分布。只要我们能够通过神经网络近似这个分布，就可以通过采样的方式生成新样本。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/24kn3mc.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/xQ6diI9.png" width="100%">
</div>

然而，生成式模型需要回答以下两个问题：

1. 如何度量真实数据分布$$p(x)$$与网络$$\pi_\theta (x)$$之间的相似程度？
2. 如何使用神经网络来表示高维数据的分布？

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/KyOsMaO.png" width="100%">
</div>

对于第一个问题，通常可以使用[KL散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)来进行处理，它度量了$$p$$和$$q$$两个概率分布之间的差异。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/67Njibg.png" width="100%">
</div>

假设$$p$$是一个已知的概率分布，则最小化$$p$$和$$q$$之间KL散度等价于对$$q$$进行最大似然估计。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/484kltM.png" width="100%">
</div>

## VAE

### 隐变量

KL散度可以用来解决度量概率分布之间相似度的问题。在此基础上，我们可以利用[变分自编码器(VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder)来对概率分布进行建模。VAE是生成式模型中的经典方法，它的核心在于构造一个服从标准正态分布的隐变量$$z \sim N(0, I)$$，且其维度要远小于真实数据$$x$$的维度。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/GT0PBdd.png" width="100%">
</div>

每个数据样本都对应着一个隐变量分布，它们之间的关系可以通过联合概率进行描述。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Xq8gzgM.png" width="100%">
</div>

在VAE模型中，数据分布$$p(x)$$和隐变量分布$$p(z)$$通过**编码器(encoder)**和**解码器(decoder)**关联在一起：

- 编码器将数据$$x$$映射为隐变量$$z$$，相当于计算条件概率$$p(z \vert x)$$。
- 解码器将隐变量$$z$$重建为数据$$x$$，相当于计算条件概率$$p(x \vert z)$$。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/os5LrLe.png" width="100%">
</div>

有了条件概率后就可以计算样本数据$$x$$在VAE网络中的概率

$$
\pi_\theta (x) = \int p_\theta (x \vert z) p (z) \ \mathrm{d} z = \mathbb{E}_{z \sim p(z)} [p_\theta (x \vert z)]
$$

进而通过最小化KL散度来进行训练。然而，需要注意的是，在计算期望$$\mathbb{E}_{z \sim p(z)} [p_\theta (x \vert z)]$$时需要对$$z$$进行采样，通常需要非常多的样本才能保证计算结果的准确性。除此之外，我们还希望数据$$x$$和隐变量$$z$$之间保持有相对有序的对应关系。因此，需要借助编码器来规范$$z$$的行为。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/09wvIqE.png" width="100%">
</div>

具体来说，在VAE中使用两个神经网络来表示条件概率：

- 解码器$$D_\theta (z)$$输出$$p_\theta (x \vert z)$$的均值，而$$p_\theta (x \vert z)$$的方差一般规定为1。
- 编码器输出$$q_\phi (z \vert x)$$的均值和方差，这里假定了$$q_\phi (z \vert x)$$服从正太分布

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZBtODYG.png" width="100%">
</div>

这样，联合概率分布$$p(x, z)$$就有两种表达方式：

- 对隐变量$$z$$进行采样并利用解码器有$$p(x, z) = p_\theta (x \vert z) p(z)$$
- 对数据$$x$$进行采样并利用编码器有$$q(x, z) = q_\phi (z \vert x) p (x)$$

两种表示方法对应着同一个分布，因此可以使用KL散度$$D_{KL} (q(x, z) \Vert p(x, z))$$作为损失函数进行训练。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/FBYt3pv.png" width="100%">
</div>

### 损失函数

把KL散度进行展开并略去只关于样本分布$$p(x)$$的部分可以得到最终的损失函数为

$$
L = \mathbb{E}_{z \sim q_\phi (z \vert x_i)} \bigg[ \log{\frac{q_\phi (z \vert x_i)}{p(x_i, z)}} \bigg]
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/jq25Sqd.png" width="100%">
</div>

实际上，上面定义的损失函数还对应着$$\log p(x)$$的下界，也即最大似然估计。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/Z1Sx5vG.png" width="100%">
</div>

继续对损失函数进行展开，可以将损失函数拆分为两部分：

$$
L = \mathbb{E}_{z \sim q_\phi (z \vert x_i)} [-\log{p_\theta (x_i \vert z)}] + D_{KL} (q_\phi (z \vert x_i) \Vert p(z))
$$

其中第一项对应着生成结果与原始数据之间的误差(重建误差)，而第二项对应着两个正态分布之间的KL散度。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/lhspieW.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/3rfe61k.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/hmlNpWm.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/cLJ6Put.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/VRwYr0M.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/I2m6Grt.png" width="100%">
</div>

### 训练与生成

推导完VAE损失函数后就可以按照通常的神经网络来进行训练。而需要进行生成时只需要从标准正态分布进行采样，并送入解码器来得到新样本。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/lU1FNvf.png" width="100%">
</div>

VAE模型的一个缺陷在于它很难生成高质量的数据。对于图像生成任务，这表现为生成的图片大多比较"糊"。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/B3ObRkt.jpg" width="100%">
</div>

## Diffusion Model

目前火热的**扩散模型(diffusion model)**是最新一代的生成式模型。与VAE相比，扩散模型能够生成更高清、更高质量的图像。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/ptH05lR.jpg" width="100%">
</div>

从数学上来说，扩散模型和VAE之间有许多相通之处。根据上一节的推导，VAE的核心在于建立数据分布和隐变量空间之间的双向映射关系。扩散模型也有类似的思想，不过在扩散模型中，随机噪声分布和数据分布是通过一系列双向映射来实现的。通过这样多步的映射，扩散模型具有更强的表达能力，生成的数据质量也更高。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/IaG3yxU.png" width="100%">
</div>

### 前向过程

扩散模型包括前向和反向两个过程，其中前向过程表示对一张给定的图片添加噪声直至它完全变成无法用肉眼识别的噪声图像。这里每一步添加噪声可以表示为一个马尔科夫链，即第$$t$$步的结果只与上一步$$t-1$$的状态有关。假设在每一步中图像都服从正态分布，则添加噪声的过程可以表示为

$$
q(x_t \vert x_{t-1}) = N(\sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
$$

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/WFghg2M.png" width="100%">
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZzsdESr.png" width="100%">
</div>

利用重参数化的技巧，添加噪声的过程可以表示为

$$
x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \varepsilon_{t-1}
$$

其中$$\varepsilon$$表示来自于标准正态分布的随机噪声。

<div align=center>
<img src="https://search.pstatic.net/common?src=https://i.imgur.com/BcrDPPV.png" width="100%">
</div>

### 反向过程

## Reference

- [Lecture 15: 生成模型](https://www.bilibili.com/video/BV1MF4m1V7e3?p=14&vd_source=7a2542c6c909b3ee1fab551277360826)