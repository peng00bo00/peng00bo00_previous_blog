---
layout: article
title: 深蓝学院-GNN课程笔记01-图论基础
tags: ["深蓝学院", "GNN"]
key: 深蓝学院-GNN-01
aside:
  toc: true
sidebar:
  nav: GNN
---

> 这个系列是深蓝学院[图深度学习理论与实践](https://www.shenlanxueyuan.com/my/course/376)的同步课程笔记。课程内容涉及图深度学习的基础理论与应用，本节主要图论的基础知识。
<!--more-->

## 图的表示

一个**图(Graph)**可以表示为$$G = \{ V, E \}$$，其中$$V = \{ v_1, ... , v_N \}$$是节点的集合, $$E = \{ e_1, ... , e_M \}$$是边的集合。

<div align=center>
<img src="https://i.imgur.com/GtXb6SN.png" width="40%">
</div>

一个图可以用**邻接矩阵(adjacency matrix)**来存储。给定一个图$$G = \{ V, E \}$$，对应的邻接矩阵可以表示为$$A = \{ 0, 1 \}^{N \times N}$$，如果$v_i$和$v_j$相邻则$A_{ij} = 1$否则$A_{ij} = 0$。显然无向图的邻接矩阵一定是对称的，上图对应的邻接矩阵为：

$$
A =
\begin{bmatrix}
0 & 1 & 0 & 1 & 1 \\
1 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 1 & 1 & 0 \\
\end{bmatrix}
$$

## 图的性质

### 度

图的性质有很多，其中最基本的性质是节点的**度(degree)**，它表示与该节点相邻的其他节点的数目：

$$
d(v_i) = \sum_{v_i \in V} \mathbb{1}_E (\{ v_i, v_j \}) = \sum_{j=1}^N A_{ij}
$$

$$
\mathbb{1}_E (\{ v_i, v_j \}) = 
\left\{
\begin{aligned}
& 1, & & (v_i, v_j) \in E \\
& 0, & & \text{otherwise}
\end{aligned}
\right.
$$

<div align=center>
<img src="https://i.imgur.com/Im1obVi.png" width="40%">
</div>

同时，图上所有节点度的和(邻接矩阵中非零元素的个数)等于图上边数量的2倍：

$$
\sum_{v_i \in V} d(v_i) = 2 \vert E \vert
$$

在度的基础上我们可以定义**邻域(neighborhood)** $N(v_i)$，它表示所有与节点$v_i$相邻的节点构成的集合。显然邻域$N(v_i)$中元素的个数等于节点的度：

$$
d(v_i) = \vert N(v_i) \vert
$$

<div align=center>
<img src="https://i.imgur.com/6IlJTE5.png" width="40%">
</div>

### 连通度

我们定义**途径(walk)**是节点与边的交替序列，从一个节点开始，以一个节点结束，其中每条边与紧邻的节点相连。途径的长度定义为途径中包含的边的数量。

<div align=center>
<img src="https://i.imgur.com/ueE9u8T.png" width="40%">
</div>

在途径的基础上可以定义**迹(trail)**和**路径(path)**，其中迹是边各不相同的途径，而路径是节点各不相同的途径。换句话说，迹中不能有相同的边儿路径中不能有相同的节点。以下图为例，$(v_1, e_4, v_4, e_5, v_5, e_6, v_1, e_1, v_2)$是一条迹但不是路径，而$(v_1, e_4, v_4, e_5, v_5, e_3, v_3)$则是一条路径。

<div align=center>
<img src="https://i.imgur.com/ueE9u8T.png" width="41%">
<img src="https://i.imgur.com/tmjeUYH.png" width="40%">
</div>

给定一个图，如果图中的任意两个节点之间都至少存在一条路径，则称这个图是一个**连通图(connected graph)**。对于下图所表示的图，如果只考虑左半部分则它是一个连通图，如果把整个图放在一起考虑则不是一个连通图。

<div align=center>
<img src="https://i.imgur.com/UpT1OJs.png" width="70%">
</div>

给定连通图中的任意两点，连接这两点的长度最小的路径被称为这两点之间的**最短路径(shortest path)**，最短路径的长度被称为两点间的距离。需要注意的是一对节点之间可能存在不止一条最短路径，如下图中$v_5$和$v_2$之间就存在2条最短路径。

<div align=center>
<img src="https://i.imgur.com/aAbz3s9.png" width="40%">
</div>

通过最短路径我们可以定义图的**直径(diameter)**为图上最远的两点间的距离，即所有最短路径中的最长路径的长度。

<div align=center>
<img src="https://i.imgur.com/C0Mqsom.png" width="40%">
</div>

### 中心性

节点的**中心性(centrality)**用来衡量节点在图上的重要程度，其中最基础的是**度中心性(degree centrality)**，它基于节点的度来测量中心性：

$$
c_d(v_i) = d(v_i) = \sum_{j=1}^N A_{ij}
$$

度中心性背后的思想在于与节点$v_i$相邻的节点越多，$v_i$在图上就越重要。

度中心性认为与节点$v_i$相邻的节点都具有相同的重要性，然而在实际中不同节点的重要性往往是不同的。在此基础上我们可以定义节点的**特征向量中心性(eigenvector centrality)**来考虑相邻节点的中心性对中心节点的贡献：

$$
c_e(v_i) = \frac{1}{\lambda} \sum_{j=1}^N A_{ij} c_e(v_j)
$$

其中$\lambda$为归一化系数。将上面的公式写成矩阵的形式不难发现特征向量中心性$c_e$实际上对应了连接矩阵$A$的特征向量，而$\lambda$为对应的特征值：

$$
\lambda c_e = A c_e
$$

显然对于给定的图可能存在多个不同的特征向量，实际中一般可取最大特征值的作为特征向量中心性。

类似地，可以定义**Katz中心性(Katz centrality)**为：

$$
c_k(v_i) = \alpha \sum_{j=1}^N A_{ij} c_k(v_j) + \beta
$$

其中常数$\beta$表示中心节点自身天然具有的重要性，而常数$\alpha$表示其他节点贡献的中心性相对于$\beta$的权重。将上面的公式写成矩阵的形式得到：

$$
c_k = \alpha A c_k + \beta
$$

$$
(I - \alpha A) c_k = \beta
$$

显然常数$\alpha$和$\beta$控制了Katz中心性的值，当$\alpha=\frac{1}{\lambda_{\text{max}}}$且$\beta = 0$时Katz中心性即为特征向量中心性。同时需要注意$\alpha$的选取：比较大的$\alpha$容易导致矩阵$I - \alpha A$出现病态的情况， 在实践中一般令$\alpha < \frac{1}{\lambda_{\text{max}}}$；而较小的$\alpha$会导致不同节点都具有相似的中心性，此时计算得到的中心性是没有意义的。

## 谱图论与图信号处理

### 拉普拉斯矩阵

**拉普拉斯矩阵(Laplacian matrix)**在图论中具有非常重要的应用，其定义为节点度矩阵与邻接矩阵的差：

$$
L = D - A
$$

由于度矩阵$D$和邻接矩阵$A$都是对称矩阵，显然拉普拉斯矩阵$L$也是对称矩阵。

通过拉普拉斯矩阵可以度量每个节点与和它相邻节点的差异。假设节点$v_i$具有的特征为$f_i$，将拉普拉斯矩阵与节点特征向量相乘得到：

$$
\begin{aligned}
h &= L f \\
&= (D - A) f \\
&= Df - Af
\end{aligned}
$$

$h$中的每一项表示节点与每个相邻节点差的和：

$$
\begin{aligned}
h_i &= d(v_i) f_i - \sum_{j=1}^N A_{ij} f_j \\
&= \sum_{v_j \in N(v_i)} (f_i - f_j)
\end{aligned}
$$

类似地，$f$关于拉普拉斯矩阵的二次型为：

$$
\begin{aligned}
f^T L f &= \sum_{v_i \in V} f_i \sum_{v_j \in N(v_i)} (f_i - f_j) \\
&= \sum_{v_i \in V} \sum_{v_j \in N(v_i)} (f_i f_i - f_i f_j) \\
&= \sum_{v_i \in V} \sum_{v_j \in N(v_i)} (\frac{1}{2} f_i f_i - f_i f_j + \frac{1}{2} f_j f_j) \\
&= \frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in N(v_i)} (f_i - f_j)^2
\end{aligned}
$$

因此$f^T L f$是相邻节点差的平方和，它表示相邻节点之间的差异。同时$f^T L f \geq 0$，这说明拉普拉斯矩阵是半正定的。更进一步可以证明拉普拉斯矩阵0特征值的个数(特征值0的重数)等于图上连通分量的数目。

### 图信号处理

我们可以为图上的每一个节点赋予一个特征，这样的图数据结构称为图信号。图信号由图$$G = \{ V, E \}$$和节点域上定义的将节点映射为特征的映射$f: V \rightarrow \mathbb{R}^d$构成。

<div align=center>
<img src="https://i.imgur.com/ySw0aiG.png" width="50%">
</div>

前面已经证明了拉普拉斯矩阵的二次型$f^T L f$度量了图信号的光滑程度，$f^T L f$越小表示图上不同节点间的差异越小，图信号变化越平滑。因此，$f^T L f$也称为信号的平滑度(频率)。

在一维信号处理中同一个信号可以在时域和频域两个域上进行表示，它们之间的转换由傅里叶变换给出：

$$
F(\xi) = \langle f(t), \exp \{ -2 \pi i t \xi \} \rangle = \int_{-\infty}^\infty f(t) \exp \{ -2 \pi i t \xi \} d t
$$

上式可以理解为将时域信号$f(t)$投影到不同频率的基函数$$\exp \{ -2 \pi i t \xi \}$$上。结合拉普拉斯算子不难发现基函数$$\exp \{ -2 \pi i t \xi \}$$是拉普拉斯算子的特征函数：

$$
\begin{aligned}
\Delta \exp \{ -2 \pi i t \xi \} &= \frac{\partial^2}{\partial t^2} \exp \{ -2 \pi i t \xi \} \\
&= -(2 \pi \xi)^2 \exp \{ -2 \pi i t \xi \}
\end{aligned}
$$

因此类似于一维信号处理，图上的傅里叶变换只需要将图信号投影到拉普拉斯矩阵的特征向量上即可：

$$
F(l) = \langle f, u_l \rangle = u_l^T f = \sum_{i=1}^N f_i u_l[i]
$$

其中$u_l$是拉普拉斯矩阵的第$l$个特征向量，对应的特征值$\lambda_l$为$u_l$的频率。

拉普拉斯矩阵的特征向量构成了图$G$的傅里叶基，频域信号$F$由图信号$f$在这些傅里叶基上的系数组成。因此**图傅里叶变换(graph Fourier transform, GFT)**的矩阵形式为：

$$
F = U^T f
$$

其中$U$的每一列均为拉普拉斯矩阵的特征向量。相应地，**图傅里叶逆变换(inverse graph Fourier transform, IGFT)**为：

$$
f = U F
$$

<div align=center>
<img src="https://i.imgur.com/8mWesye.png" width="40%">
<img src="https://i.imgur.com/oqHFJaU.png" width="40%">
</div>

## 复杂图